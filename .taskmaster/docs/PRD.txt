# Product Requirements Document (PRD): llm-finetune-supportbot

## 1. Overview

This project fine‑tunes a small open LLM on **tech‑support style conversations** and evaluates whether helpfulness and accuracy improve versus a base model. It includes a minimal repo scaffold, scripts for evaluation, automated tests, and lightweight demos (CLI and FastAPI). The focus is on measurable answer quality, hallucination reduction, and visibility into training cost/time.  &#x20;

**Positioning:** A compact, reproducible pipeline to fine‑tune and assess small LLMs for tech‑support use cases, using standard open‑source tooling.&#x20;

## 2. Problem Statement

Teams needing support‑style assistants require models that answer accurately and avoid hallucinations while being inexpensive to train and serve. This project addresses that by enabling small‑model fine‑tuning and evaluation against defined metrics (answer quality, hallucination rate, cost/time). Why now: increased practicality of PEFT/LoRA and 4‑bit training (BitsAndBytes) on modest hardware plus mature HF tooling. &#x20;

## 3. Goals & Objectives

* Establish a **reproducible fine‑tune + eval** workflow for small open LLMs on support dialogs.&#x20;
* **Quantify** improvements via win‑rate vs. base model on 100 eval questions and **track hallucination rate**.&#x20;
* Provide **quickstart** and **demos** (CLI & FastAPI) to validate end‑to‑end usability. &#x20;
* Ship **tests** and (planned) **CI checks** to guard quality. &#x20;

**Scope boundaries**

* In: fine‑tuning pipeline, evaluation scripts and results, CLI/FastAPI demos, basic tests. &#x20;
* Out (not stated in README): production deployment, dataset sourcing/labeling details, specific model/hardware choices. (See Open Questions.)

## 4. Target Users & Use Cases

**Primary Users:** Contributors who need to fine‑tune and evaluate a small LLM for tech‑support Q\&A using open‑source tooling (implied by README content).&#x20;

**Key Use Cases (table):**

| User/Persona   | Scenario                         | Trigger                 | Expected Outcome                                                          |
| -------------- | -------------------------------- | ----------------------- | ------------------------------------------------------------------------- |
| Repo user      | Spin up environment and run demo | Follows Quickstart      | Demo script executes end‑to‑end without error (CLI Q\&A works).           |
| Evaluator      | Reproduce metrics                | Runs scripts in `eval/` | CSV/JSON results written under `results/` and usable for summary tables.  |
| QA/Contributor | Verify baseline correctness      | Runs `pytest -q`        | Tests pass locally to validate core behaviors.                            |
| API integrator | Try service‑style interface      | Starts FastAPI demo     | Can issue a request and receive a model response (qualitative check).     |

## 5. Features & Requirements

### 5.1 Functional Requirements

| ID    | Title                           | Priority (MoSCoW) | Description                                                              | Acceptance Criteria                                                                                                 | Source (README section) |
| ----- | ------------------------------- | ----------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------- | ----------------------- |
| FR-1  | Quickstart environment setup    | Must              | Provide commands to create venv, install deps, and run demo.             | Running the listed commands completes without error on a clean machine. `python demo.py` executes.                  | Quickstart              |
| FR-2  | Evaluation scripts              | Must              | Scripts in `eval/` reproduce metrics and save to `results/` in CSV/JSON. | Executing documented eval scripts produces new files under `results/` (CSV/JSON) consistent with README summaries.  | Evaluation              |
| FR-3  | Tests runnable via pytest       | Must              | Project includes tests that run with `pytest -q`.                        | `pytest -q` returns successful status locally.                                                                      | Tests                   |
| FR-4  | CLI demo                        | Should            | Provide CLI Q\&A demonstration.                                          | A user can ask at least one question and receive a model answer via CLI.                                            | Demos                   |
| FR-5  | FastAPI demo                    | Should            | Provide a FastAPI endpoint demonstration.                                | Starting the FastAPI app allows sending a request and receiving a response.                                         | Demos                   |
| FR-6  | Before/after examples           | Should            | Include qualitative examples showing base vs. fine‑tuned answers.        | Repository contains at least one before/after pair consumable in README or demo.                                    | Demos                   |
| FR-7  | Repo structure                  | Must              | Maintain the listed top‑level structure.                                 | The repository contains `src/`, `demo.py`, `eval/`, `results/`, `tests/`, `requirements.txt`, `README.md`.          | Structure               |
| FR-8  | Define baseline & targets       | Must              | Specify baseline and target values for success metrics.                  | Documented baseline and target metrics exist for win‑rate, hallucination rate, and cost/time.                       | Roadmap                 |
| FR-9  | CI checks                       | Should            | Add CI to automate tests/linters per roadmap.                            | CI pipeline runs on PR and surfaces pass/fail for tests; status badge or equivalent evidence present.               | Roadmap                 |
| FR-10 | Document limitations/next steps | Should            | Provide a section documenting limitations and future work.               | README includes a “Limitations & Next Steps” section.                                                               | Roadmap                 |
| FR-11 | License                         | Must              | Provide MIT license and respect upstream licenses.                       | LICENSE or README states MIT; references to upstream license compliance present.                                    | License                 |

### 5.2 Non‑Functional Requirements

| ID    | Quality Attribute       | Requirement                                                              | Measure/Target                                                                       | Rationale                                                            | Source (README section) |
| ----- | ----------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | -------------------------------------------------------------------- | ----------------------- |
| NFR-1 | Reproducibility         | Evaluation must be reproducible via scripts under `eval/`.               | Ability to regenerate the same CSV/JSON under `results/` following documented steps. | Ensure credible comparisons across runs.                             | Evaluation              |
| NFR-2 | Tooling Standards       | Use Python + PyTorch + HF Transformers with PEFT/LoRA and BitsAndBytes.  | Dependencies listed and installable via `requirements.txt`.                          | Align with ecosystem standards for small‑model fine‑tuning.          | Tech Stack              |
| NFR-3 | Cost/Time Observability | Track training cost and wall‑clock time.                                 | Reported alongside results for each run (exact format TBD).                          | Explicit in success metrics to inform practicality.                  | Success Metrics         |
| NFR-4 | Quality Measurement     | Measure answer quality (win‑rate on 100 eval Qs) and hallucination rate. | Present numeric win‑rate and hallucination rate in results summary.                  | Core value proposition is quality uplift with fewer hallucinations.  | Success Metrics         |
| NFR-5 | Compliance              | Respect upstream licenses.                                               | Presence of attribution/notice; no conflicting license usage.                        | Legal hygiene for dependencies.                                      | License                 |

## 6. Success Metrics

| Metric                             | Type (Leading/Lagging) | Baseline | Target  | Timeframe | Data Source                                              |
| ---------------------------------- | ---------------------- | -------- | ------- | --------- | -------------------------------------------------------- |
| Win‑rate vs. base on 100 eval Qs   | Lagging                | **TBD**  | **TBD** | **TBD**   | `eval/` outputs → `results/` CSV/JSON.                   |
| Hallucination rate (manual rubric) | Lagging                | **TBD**  | **TBD** | **TBD**   | Same as above; rubric documented in repo (to be added).  |
| Training cost & time               | Leading                | **TBD**  | **TBD** | **TBD**   | Logged during training and summarized in `results/`.     |

> Note: Baselines/targets/timeframes are intentionally **TBD** per roadmap item “Define baseline & target metrics.”&#x20;

## 7. Risks & Assumptions

**Risks**

| Risk                                              | Impact | Likelihood | Mitigation/Contingency                                    |
| ------------------------------------------------- | ------ | ---------- | --------------------------------------------------------- |
| Undefined baselines/targets delay eval usefulness | Medium | High       | Prioritize FR‑8 to define metrics early.                  |
| Hallucination rubric ambiguity                    | Medium | Medium     | Document rubric and examples with before/after section.   |
| CI not implemented                                | Medium | Medium     | Implement FR‑9; start with tests in `tests/`.             |

**Assumptions**

* Small open LLMs + PEFT/LoRA are sufficient to improve support‑style responses (implied by project goal).&#x20;
* Users can run Python tooling locally and install dependencies via `requirements.txt`.&#x20;

**Dependencies**

* Python, PyTorch, **Transformers**, **datasets**, **peft**, **trl**, **BitsAndBytes**.&#x20;

## 8. Open Questions

* Which **base model(s)** and size(s) are in scope for “small” (e.g., parameter count)? Decision needed to reproduce results and control cost/time. (Affects FR‑2, NFR‑3/4.)
* What **dataset(s)** and preprocessing steps define “tech‑support style conversations”? Decision affects eval validity and reproducibility. (Affects FR‑2, NFR‑1/4.)
* What **hardware assumptions** (GPU type/memory) are required to complete Quickstart and training? Decision affects onboarding and cost/time metrics. (Affects FR‑1, NFR‑3.)
* Exact **evaluation procedure**: prompt format, sampling params, and the **manual rubric** for hallucination. Decision needed to standardize CSV/JSON outputs. (Affects FR‑2, NFR‑4.)
* **CI scope** (tests/linters/format): which checks to include initially? (Affects FR‑9.)
* Where/how to **document limitations & next steps** (new section in README vs. separate doc)? (Affects FR‑10.)
* Any **license notices** required for specific upstream assets beyond MIT statement? (Affects NFR‑5, FR‑11.)

---

## Validation Checklist

* [x] All requirements have IDs and MoSCoW priorities.
* [x] Each functional requirement has clear, objective acceptance criteria.
* [x] Non‑functional requirements specify measurable targets where available.
* [x] Every requirement cites a README source section or is flagged as an Open Question.
* [x] No content invented beyond the README.

---

## Input (from README)

```
# Fine-tune a Small LLM on Support Data

Repo: llm-finetune-supportbot • Last Updated: 2025-08-19

🎯 Goal
Train a small open LLM on tech-support style conversations and evaluate improvements in helpfulness and accuracy.

🧱 Tech Stack
Python, PyTorch, Hugging Face Transformers, PEFT/LoRA, BitsAndBytes

🔗 Upstream / Tools Used
transformers, datasets, peft, trl

✅ Success Metrics
- Answer quality (win rate vs. base model on 100 eval questions)
- Hallucination rate (manual rubric)
- Training cost & time

🚀 Quickstart
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
python demo.py

📊 Evaluation
- Scripts in eval/ reproduce metrics.
- Results saved to results/ as CSV/JSON, summarized in README tables.

🧪 Tests
pytest -q

📦 Structure
src/, demo.py, eval/, results/, tests/, requirements.txt, README.md

📸 Demos
CLI Q&A, FastAPI endpoint, Before/after qualitative examples

🗺️ Roadmap
Define baseline & target metrics, Implement MVP, Add CI checks, Document limitations & next steps

⚖️ License
MIT (adjust as needed). Respect upstream licenses.
```

