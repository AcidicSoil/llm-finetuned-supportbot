# Product Requirements Document (PRD): llm-finetune-supportbot

## 1. Overview

This project fineâ€‘tunes a small open LLM on **techâ€‘support style conversations** and evaluates whether helpfulness and accuracy improve versus a base model. It includes a minimal repo scaffold, scripts for evaluation, automated tests, and lightweight demos (CLI and FastAPI). The focus is on measurable answer quality, hallucination reduction, and visibility into training cost/time.  &#x20;

**Positioning:** A compact, reproducible pipeline to fineâ€‘tune and assess small LLMs for techâ€‘support use cases, using standard openâ€‘source tooling.&#x20;

## 2. Problem Statement

Teams needing supportâ€‘style assistants require models that answer accurately and avoid hallucinations while being inexpensive to train and serve. This project addresses that by enabling smallâ€‘model fineâ€‘tuning and evaluation against defined metrics (answer quality, hallucination rate, cost/time). Why now: increased practicality of PEFT/LoRA and 4â€‘bit training (BitsAndBytes) on modest hardware plus mature HF tooling. &#x20;

## 3. Goals & Objectives

* Establish a **reproducible fineâ€‘tune + eval** workflow for small open LLMs on support dialogs.&#x20;
* **Quantify** improvements via winâ€‘rate vs. base model on 100 eval questions and **track hallucination rate**.&#x20;
* Provide **quickstart** and **demos** (CLI & FastAPI) to validate endâ€‘toâ€‘end usability. &#x20;
* Ship **tests** and (planned) **CI checks** to guard quality. &#x20;

**Scope boundaries**

* In: fineâ€‘tuning pipeline, evaluation scripts and results, CLI/FastAPI demos, basic tests. &#x20;
* Out (not stated in README): production deployment, dataset sourcing/labeling details, specific model/hardware choices. (See Open Questions.)

## 4. Target Users & Use Cases

**Primary Users:** Contributors who need to fineâ€‘tune and evaluate a small LLM for techâ€‘support Q\&A using openâ€‘source tooling (implied by README content).&#x20;

**Key Use Cases (table):**

| User/Persona   | Scenario                         | Trigger                 | Expected Outcome                                                          |
| -------------- | -------------------------------- | ----------------------- | ------------------------------------------------------------------------- |
| Repo user      | Spin up environment and run demo | Follows Quickstart      | Demo script executes endâ€‘toâ€‘end without error (CLI Q\&A works).           |
| Evaluator      | Reproduce metrics                | Runs scripts in `eval/` | CSV/JSON results written under `results/` and usable for summary tables.  |
| QA/Contributor | Verify baseline correctness      | Runs `pytest -q`        | Tests pass locally to validate core behaviors.                            |
| API integrator | Try serviceâ€‘style interface      | Starts FastAPI demo     | Can issue a request and receive a model response (qualitative check).     |

## 5. Features & Requirements

### 5.1 Functional Requirements

| ID    | Title                           | Priority (MoSCoW) | Description                                                              | Acceptance Criteria                                                                                                 | Source (README section) |
| ----- | ------------------------------- | ----------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------- | ----------------------- |
| FR-1  | Quickstart environment setup    | Must              | Provide commands to create venv, install deps, and run demo.             | Running the listed commands completes without error on a clean machine. `python demo.py` executes.                  | Quickstart              |
| FR-2  | Evaluation scripts              | Must              | Scripts in `eval/` reproduce metrics and save to `results/` in CSV/JSON. | Executing documented eval scripts produces new files under `results/` (CSV/JSON) consistent with README summaries.  | Evaluation              |
| FR-3  | Tests runnable via pytest       | Must              | Project includes tests that run with `pytest -q`.                        | `pytest -q` returns successful status locally.                                                                      | Tests                   |
| FR-4  | CLI demo                        | Should            | Provide CLI Q\&A demonstration.                                          | A user can ask at least one question and receive a model answer via CLI.                                            | Demos                   |
| FR-5  | FastAPI demo                    | Should            | Provide a FastAPI endpoint demonstration.                                | Starting the FastAPI app allows sending a request and receiving a response.                                         | Demos                   |
| FR-6  | Before/after examples           | Should            | Include qualitative examples showing base vs. fineâ€‘tuned answers.        | Repository contains at least one before/after pair consumable in README or demo.                                    | Demos                   |
| FR-7  | Repo structure                  | Must              | Maintain the listed topâ€‘level structure.                                 | The repository contains `src/`, `demo.py`, `eval/`, `results/`, `tests/`, `requirements.txt`, `README.md`.          | Structure               |
| FR-8  | Define baseline & targets       | Must              | Specify baseline and target values for success metrics.                  | Documented baseline and target metrics exist for winâ€‘rate, hallucination rate, and cost/time.                       | Roadmap                 |
| FR-9  | CI checks                       | Should            | Add CI to automate tests/linters per roadmap.                            | CI pipeline runs on PR and surfaces pass/fail for tests; status badge or equivalent evidence present.               | Roadmap                 |
| FR-10 | Document limitations/next steps | Should            | Provide a section documenting limitations and future work.               | README includes a â€œLimitations & Next Stepsâ€ section.                                                               | Roadmap                 |
| FR-11 | License                         | Must              | Provide MIT license and respect upstream licenses.                       | LICENSE or README states MIT; references to upstream license compliance present.                                    | License                 |

### 5.2 Nonâ€‘Functional Requirements

| ID    | Quality Attribute       | Requirement                                                              | Measure/Target                                                                       | Rationale                                                            | Source (README section) |
| ----- | ----------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | -------------------------------------------------------------------- | ----------------------- |
| NFR-1 | Reproducibility         | Evaluation must be reproducible via scripts under `eval/`.               | Ability to regenerate the same CSV/JSON under `results/` following documented steps. | Ensure credible comparisons across runs.                             | Evaluation              |
| NFR-2 | Tooling Standards       | Use Python + PyTorch + HF Transformers with PEFT/LoRA and BitsAndBytes.  | Dependencies listed and installable via `requirements.txt`.                          | Align with ecosystem standards for smallâ€‘model fineâ€‘tuning.          | Tech Stack              |
| NFR-3 | Cost/Time Observability | Track training cost and wallâ€‘clock time.                                 | Reported alongside results for each run (exact format TBD).                          | Explicit in success metrics to inform practicality.                  | Success Metrics         |
| NFR-4 | Quality Measurement     | Measure answer quality (winâ€‘rate on 100 eval Qs) and hallucination rate. | Present numeric winâ€‘rate and hallucination rate in results summary.                  | Core value proposition is quality uplift with fewer hallucinations.  | Success Metrics         |
| NFR-5 | Compliance              | Respect upstream licenses.                                               | Presence of attribution/notice; no conflicting license usage.                        | Legal hygiene for dependencies.                                      | License                 |

## 6. Success Metrics

| Metric                             | Type (Leading/Lagging) | Baseline | Target  | Timeframe | Data Source                                              |
| ---------------------------------- | ---------------------- | -------- | ------- | --------- | -------------------------------------------------------- |
| Winâ€‘rate vs. base on 100 eval Qs   | Lagging                | **TBD**  | **TBD** | **TBD**   | `eval/` outputs â†’ `results/` CSV/JSON.                   |
| Hallucination rate (manual rubric) | Lagging                | **TBD**  | **TBD** | **TBD**   | Same as above; rubric documented in repo (to be added).  |
| Training cost & time               | Leading                | **TBD**  | **TBD** | **TBD**   | Logged during training and summarized in `results/`.     |

> Note: Baselines/targets/timeframes are intentionally **TBD** per roadmap item â€œDefine baseline & target metrics.â€&#x20;

## 7. Risks & Assumptions

**Risks**

| Risk                                              | Impact | Likelihood | Mitigation/Contingency                                    |
| ------------------------------------------------- | ------ | ---------- | --------------------------------------------------------- |
| Undefined baselines/targets delay eval usefulness | Medium | High       | Prioritize FRâ€‘8 to define metrics early.                  |
| Hallucination rubric ambiguity                    | Medium | Medium     | Document rubric and examples with before/after section.   |
| CI not implemented                                | Medium | Medium     | Implement FRâ€‘9; start with tests in `tests/`.             |

**Assumptions**

* Small open LLMs + PEFT/LoRA are sufficient to improve supportâ€‘style responses (implied by project goal).&#x20;
* Users can run Python tooling locally and install dependencies via `requirements.txt`.&#x20;

**Dependencies**

* Python, PyTorch, **Transformers**, **datasets**, **peft**, **trl**, **BitsAndBytes**.&#x20;

## 8. Open Questions

* Which **base model(s)** and size(s) are in scope for â€œsmallâ€ (e.g., parameter count)? Decision needed to reproduce results and control cost/time. (Affects FRâ€‘2, NFRâ€‘3/4.)
* What **dataset(s)** and preprocessing steps define â€œtechâ€‘support style conversationsâ€? Decision affects eval validity and reproducibility. (Affects FRâ€‘2, NFRâ€‘1/4.)
* What **hardware assumptions** (GPU type/memory) are required to complete Quickstart and training? Decision affects onboarding and cost/time metrics. (Affects FRâ€‘1, NFRâ€‘3.)
* Exact **evaluation procedure**: prompt format, sampling params, and the **manual rubric** for hallucination. Decision needed to standardize CSV/JSON outputs. (Affects FRâ€‘2, NFRâ€‘4.)
* **CI scope** (tests/linters/format): which checks to include initially? (Affects FRâ€‘9.)
* Where/how to **document limitations & next steps** (new section in README vs. separate doc)? (Affects FRâ€‘10.)
* Any **license notices** required for specific upstream assets beyond MIT statement? (Affects NFRâ€‘5, FRâ€‘11.)

---

## Validation Checklist

* [x] All requirements have IDs and MoSCoW priorities.
* [x] Each functional requirement has clear, objective acceptance criteria.
* [x] Nonâ€‘functional requirements specify measurable targets where available.
* [x] Every requirement cites a README source section or is flagged as an Open Question.
* [x] No content invented beyond the README.

---

## Input (from README)

```
# Fine-tune a Small LLM on Support Data

Repo: llm-finetune-supportbot â€¢ Last Updated: 2025-08-19

ğŸ¯ Goal
Train a small open LLM on tech-support style conversations and evaluate improvements in helpfulness and accuracy.

ğŸ§± Tech Stack
Python, PyTorch, Hugging Face Transformers, PEFT/LoRA, BitsAndBytes

ğŸ”— Upstream / Tools Used
transformers, datasets, peft, trl

âœ… Success Metrics
- Answer quality (win rate vs. base model on 100 eval questions)
- Hallucination rate (manual rubric)
- Training cost & time

ğŸš€ Quickstart
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
python demo.py

ğŸ“Š Evaluation
- Scripts in eval/ reproduce metrics.
- Results saved to results/ as CSV/JSON, summarized in README tables.

ğŸ§ª Tests
pytest -q

ğŸ“¦ Structure
src/, demo.py, eval/, results/, tests/, requirements.txt, README.md

ğŸ“¸ Demos
CLI Q&A, FastAPI endpoint, Before/after qualitative examples

ğŸ—ºï¸ Roadmap
Define baseline & target metrics, Implement MVP, Add CI checks, Document limitations & next steps

âš–ï¸ License
MIT (adjust as needed). Respect upstream licenses.
```

