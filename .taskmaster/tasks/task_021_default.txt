# Task ID: 21
# Title: Expand Evaluation Suite and Error Analysis
# Status: pending
# Dependencies: 10
# Priority: medium
# Description: Add more evaluation questions and tools for qualitative error analysis.
# Details:
Curate additional evaluation sets for different domains. Enhance the results output to include before/after examples and categorize errors into buckets (e.g., 'wrong_fact', 'out_of_scope').

# Test Strategy:
Run the enhanced evaluation and check that the output includes the new error buckets and qualitative examples.

# Subtasks:
## 1. Curate additional evaluation sets [pending]
### Dependencies: None
### Description: Add at least two new JSON/JSONL evaluation suites for different support domains.
### Details:
Create `eval/suites/support_billing.jsonl` and `eval/suites/support_setup.jsonl` with fields `{id, inputs:{question, context?}, outputs:{answer?}, meta:{tags[]}}`. Ensure coverage of varied intents and difficulty.

## 2. Define error classification schema and rubric [pending]
### Dependencies: None
### Description: Introduce structured error categories and a rubric for annotation.
### Details:
Add `src/eval_schema.py` with `Enum ErrorType { REFUSAL, HALLUCINATION, STYLE, OTHER }` and rubric notes in `eval/README.md`. Provide examples and decision rules.

## 3. Annotate eval outputs with error category [pending]
### Dependencies: None
### Description: Modify evaluation to capture error categories per response.
### Details:
Extend `scripts/eval.py` to optionally apply a rule-based tagger (using the rubric) and add `error_type` to each record. Leave hooks for manual review or future ML-based tagging.

## 4. Aggregate and report error statistics [pending]
### Dependencies: None
### Description: Update results summary to include error-type aggregates.
### Details:
Extend results writer to compute counts and rates per `ErrorType` and print a summary table. Save an `error_stats.json` with aggregates per suite and overall.

