{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Quickstart Environment Setup",
        "description": "Create and validate a reproducible Python environment that runs the demo end-to-end on a clean machine.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Provision virtualenv, install dependencies from requirements.txt, and verify demo.py executes successfully. Document platform notes (Linux/macOS/Windows), GPU optionality, and minimal hardware specs.",
        "testStrategy": "Run a smoke test script that creates a venv, installs deps, executes demo.py with a dummy prompt, and asserts non-empty output and zero exit code.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create virtual environment",
            "description": "Add make/PowerShell helpers to create/activate .venv consistently across OSes.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "Running the helper creates .venv and activation succeeds without errors."
          },
          {
            "id": 2,
            "title": "Install dependencies",
            "description": "Pin and install requirements.txt; verify CUDA/CPU variants as applicable.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "pip install completes with zero errors and compatible versions are resolved."
          },
          {
            "id": 3,
            "title": "Run demo smoke test",
            "description": "Execute python demo.py with a sample question and capture output.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Command exits 0 and prints a response string > 10 characters."
          },
          {
            "id": 4,
            "title": "Document Quickstart",
            "description": "Add Quickstart section with exact commands and troubleshooting tips.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "README Quickstart produces a working run on a fresh machine following steps verbatim."
          },
          {
            "id": 5,
            "title": "Hardware & OS matrix",
            "description": "List tested OS/GPU configurations and minimum specs to complete Quickstart.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "Matrix table shows at least 2 OS variants and states GPU optionality/VRAM guidance."
          }
        ]
      },
      {
        "id": 2,
        "title": "Repository Structure & Scaffolding",
        "description": "Ensure required folders/files exist and are populated with minimal working content.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "Create src/, eval/, results/, tests/, demo.py, requirements.txt, and a structured README. Provide sample config files and placeholders where applicable.",
        "testStrategy": "Unit test asserts presence of required paths and lints template files for syntax; CI job fails if structure is missing.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create directory scaffold",
            "description": "Generate src/, eval/, results/, tests/ with .gitkeep as needed.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "git status shows directories tracked; tree matches PRD structure list."
          },
          {
            "id": 2,
            "title": "Add demo.py",
            "description": "Minimal CLI entrypoint that loads a base model and answers a prompt.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Running python demo.py returns an answer without exceptions."
          },
          {
            "id": 3,
            "title": "Seed requirements.txt",
            "description": "Include torch, transformers, datasets, peft, trl, bitsandbytes with compatible pins.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "pip install -r requirements.txt succeeds on a clean env."
          },
          {
            "id": 4,
            "title": "README skeleton",
            "description": "Add sections: Goal, Tech Stack, Quickstart, Evaluation, Tests, Structure, Demos, Roadmap, License.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "README renders with all listed sections and working code blocks."
          },
          {
            "id": 5,
            "title": "Pre-commit lint config",
            "description": "Optional: basic ruff/black/isort config files to standardize formatting.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "Running linters yields 0 errors on fresh checkout."
          }
        ]
      },
      {
        "id": 3,
        "title": "Fine-tuning Pipeline (PEFT/LoRA, 4-bit)",
        "description": "Implement training scripts to fine-tune a small open LLM for tech-support dialogs with PEFT/LoRA and optional 4-bit quantization.",
        "status": "pending",
        "dependencies": [
          1,
          2
        ],
        "priority": "high",
        "details": "Use Hugging Face Transformers + PEFT/TRL with BitsAndBytes for 4-bit. Support dataset loading, tokenization, training args, checkpointing, and inference adapter merge/load.",
        "testStrategy": "Run a short training (few steps) on a tiny subset; assert checkpoints saved and generated answer differs from base on at least one prompt.",
        "subtasks": [
          {
            "id": 1,
            "title": "Select base model(s)",
            "description": "Decide parameter range and default checkpoint for \"small\" model category.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "Model name(s) documented; loading succeeds on test machine."
          },
          {
            "id": 2,
            "title": "Dataset loader & preprocessing",
            "description": "Implement scripts to load tech-support dialogs and format prompt/response pairs.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "CLI produces tokenized dataset stats and sample batch preview JSON."
          },
          {
            "id": 3,
            "title": "LoRA config & 4-bit setup",
            "description": "Enable PEFT adapters and BitsAndBytes quantization; expose key hyperparameters.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Training starts with adapters enabled and GPU memory < target threshold documented."
          },
          {
            "id": 4,
            "title": "Training & checkpointing",
            "description": "Train for N steps, save adapter checkpoints, and log metrics periodically.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "Checkpoints saved under runs/ or outputs/; logs include loss and steps."
          },
          {
            "id": 5,
            "title": "Adapter load/merge for inference",
            "description": "Script to load fine-tuned adapters for inference and optional weight merge.",
            "status": "pending",
            "dependencies": [
              4
            ],
            "acceptanceCriteria": "Inference script loads adapter and returns an answer without error."
          }
        ]
      },
      {
        "id": 4,
        "title": "Evaluation Suite & Results Artifacts",
        "description": "Provide scripts to evaluate base vs. fine-tuned models on 100 questions, compute win-rate and hallucination rate, and write CSV/JSON under results/.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3
        ],
        "priority": "high",
        "details": "Include prompt templates, sampling params, rubric hooks, and summary table generation. Log training cost/time and surface in outputs.",
        "testStrategy": "Run eval on a 10-question subset in CI; assert outputs exist, schema matches, and metrics parse without NaN.",
        "subtasks": [
          {
            "id": 1,
            "title": "Define eval data split",
            "description": "Create/ingest 100 Qs representing tech-support scenarios with references if available.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "A fixed 100-item JSON/CSV file exists with schema documented."
          },
          {
            "id": 2,
            "title": "Prompt & sampling config",
            "description": "Standardize temperature, top_p, max_tokens, and prompt format for fair comparison.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Config file checked into repo and consumed by eval scripts."
          },
          {
            "id": 3,
            "title": "Win-rate computation",
            "description": "Implement pairwise comparison or rubric scoring pipeline for base vs. fine-tuned.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "CSV/JSON includes per-item scores and aggregate win-rate %."
          },
          {
            "id": 4,
            "title": "Hallucination rubric",
            "description": "Implement manual rubric capture form and aggregation logic.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Results include hallucination rate with n and definitions documented."
          },
          {
            "id": 5,
            "title": "Cost & time logging",
            "description": "Capture wall-clock and (if using paid endpoints) cost; summarize in results.",
            "status": "pending",
            "dependencies": [
              3,
              4
            ],
            "acceptanceCriteria": "Summary JSON reports duration and estimated cost per run."
          }
        ]
      },
      {
        "id": 5,
        "title": "Test Suite (pytest)",
        "description": "Ship unit and integration tests to validate core behaviors across setup, training, and evaluation.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3
        ],
        "priority": "high",
        "details": "Provide fast tests runnable via pytest -q, including dataset loader, tokenization, tiny-step training, and eval artifact schema checks.",
        "testStrategy": "CI runs pytest -q on CPU-only mode with mocks/fakes where needed; tests must complete in < 5 minutes.",
        "subtasks": [
          {
            "id": 1,
            "title": "Unit tests: utilities",
            "description": "Test config parsing, path utils, and seed control for determinism.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "All utility tests pass locally and in CI."
          },
          {
            "id": 2,
            "title": "Dataset tests",
            "description": "Validate schema, tokenization, and truncation/padding behavior.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Loader returns batches with expected shapes and attention masks."
          },
          {
            "id": 3,
            "title": "Tiny training test",
            "description": "Run 5-10 steps with small batch to ensure loss decreases and checkpoints write.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Loss at final step < first step; checkpoint dir exists."
          },
          {
            "id": 4,
            "title": "Eval artifact schema test",
            "description": "Assert CSV/JSON fields exist for metrics (win_rate, hallucination_rate, duration).",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "pydantic or jsonschema validation passes for results files."
          },
          {
            "id": 5,
            "title": "Demo smoke tests",
            "description": "Exercise CLI and API endpoints with a short request and assert 200/zero exit code.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "CLI returns string; API GET/POST returns 200 with non-empty JSON."
          }
        ]
      },
      {
        "id": 6,
        "title": "CLI Demo",
        "description": "Provide a command-line Q&A demo showcasing base and fine-tuned model responses.",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "details": "Interactive CLI with flags to select model (base/fine-tuned), choose sampling params, and save transcripts.",
        "testStrategy": "Run CLI in non-interactive mode with a canned prompt; assert output file and stdout contain response.",
        "subtasks": [
          {
            "id": 1,
            "title": "Argument parser",
            "description": "Add flags for --model, --temperature, --top-p, --max-tokens, --save.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "--help lists all flags and defaults; invalid args error cleanly."
          },
          {
            "id": 2,
            "title": "Base vs fine-tuned toggle",
            "description": "Switch between base and adapter-augmented model for side-by-side checks.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Two outputs saved with metadata identifying model selection."
          },
          {
            "id": 3,
            "title": "Transcript logging",
            "description": "Optionally save Q&A runs under results/transcripts/ with timestamps.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Files created with ISO timestamps and prompt/response payloads."
          },
          {
            "id": 4,
            "title": "Error handling",
            "description": "Graceful failures for OOM, missing model, or bad config with actionable messages.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Known failure modes produce friendly error text and non-zero exit codes."
          },
          {
            "id": 5,
            "title": "README demo section",
            "description": "Add usage examples and sample outputs for the CLI demo.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "Users can copy/paste commands and reproduce outputs shown."
          }
        ]
      },
      {
        "id": 7,
        "title": "FastAPI Demo",
        "description": "Expose a minimal REST endpoint that returns model answers for a given question.",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "details": "FastAPI app with POST /answer accepting {question, model, params} and returning JSON with answer and metadata.",
        "testStrategy": "Use TestClient to POST a sample payload; assert 200 and JSON schema with answer length > 10.",
        "subtasks": [
          {
            "id": 1,
            "title": "Project layout",
            "description": "Place app in src/api with main.py and router; add uvicorn entrypoint.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "uvicorn src.api.main:app --reload starts without errors."
          },
          {
            "id": 2,
            "title": "Request/response models",
            "description": "Define pydantic schemas for input params and output payload.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Invalid input returns 422 with clear validation messages."
          },
          {
            "id": 3,
            "title": "Model routing",
            "description": "Wire base vs fine-tuned model selection and sampling params.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Endpoint returns distinct answers for different model selections."
          },
          {
            "id": 4,
            "title": "Logging & timing",
            "description": "Log request duration and basic telemetry for observability.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "Response includes duration_ms field; logs show start/stop timestamps."
          },
          {
            "id": 5,
            "title": "API docs & example client",
            "description": "Enable /docs and provide a curl/httpie example in README.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "OpenAPI UI renders; example curl returns 200 with sample answer."
          }
        ]
      },
      {
        "id": 8,
        "title": "Define Baselines, Targets, and Timeframes",
        "description": "Decide and document baseline/target values and timelines for win-rate, hallucination rate, and training cost/time.",
        "status": "pending",
        "dependencies": [
          4
        ],
        "priority": "high",
        "details": "Collaborate to set numeric goals, capture rationale, and record them in the README and config. Tie to eval outputs for traceability.",
        "testStrategy": "Schema check ensures metrics.yaml/json contains fields {baseline, target, timeframe} for each metric and README references them.",
        "subtasks": [
          {
            "id": 1,
            "title": "Collect baseline runs",
            "description": "Execute base-model eval to establish observed baseline metrics.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "Baseline values stored under results/ with timestamp and commit hash."
          },
          {
            "id": 2,
            "title": "Set targets",
            "description": "Define improvement thresholds (e.g., +X% win-rate, -Y% hallucinations).",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Targets documented with rationale and feasibility notes."
          },
          {
            "id": 3,
            "title": "Define timeframes",
            "description": "Attach dates/iterations for achieving targets and re-evaluation cadence.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "Timeframes captured in metrics config and referenced in Roadmap."
          },
          {
            "id": 4,
            "title": "README metrics section",
            "description": "Publish metrics table with baseline/target/timeframe and data source references.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "Rendered table matches config and links to results artifacts."
          },
          {
            "id": 5,
            "title": "Review & sign-off",
            "description": "Stakeholder review of metrics and acceptance of success criteria.",
            "status": "pending",
            "dependencies": [
              4
            ],
            "acceptanceCriteria": "Issue approved with recorded reviewers and decision log entry."
          }
        ]
      },
      {
        "id": 9,
        "title": "Continuous Integration (CI)",
        "description": "Add a CI pipeline to run linters and pytest; optionally build small eval subset.",
        "status": "pending",
        "dependencies": [
          5,
          8
        ],
        "priority": "medium",
        "details": "GitHub Actions (or equivalent) workflow that sets up Python, caches deps, runs lint, pytest -q, and executes a fast eval of 10 items to validate artifacts.",
        "testStrategy": "Open a PR to trigger CI; require all jobs green. Introduce a seeded failure to ensure CI catches issues.",
        "subtasks": [
          {
            "id": 1,
            "title": "Author workflow YAML",
            "description": "Create ci.yml with Python matrix and cache steps.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "Workflow appears in Actions and runs on pull_request and push."
          },
          {
            "id": 2,
            "title": "Lint & type check",
            "description": "Run ruff/black/isort and optional mypy; fail on violations.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "CI job fails if formatting or types are incorrect."
          },
          {
            "id": 3,
            "title": "Run tests",
            "description": "Execute pytest -q with minimal resources and report JUnit summary.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "acceptanceCriteria": "All tests pass in < 5 minutes on default runner."
          },
          {
            "id": 4,
            "title": "Eval smoke job",
            "description": "Optional: run 10-question eval to validate results schema generation.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "CI artifact upload includes CSV/JSON with expected fields."
          },
          {
            "id": 5,
            "title": "Status badge",
            "description": "Add CI status badge to README.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Badge renders and reflects latest workflow status."
          }
        ]
      },
      {
        "id": 10,
        "title": "Documentation: Limitations, Next Steps, and License",
        "description": "Document known limitations, future work, and license/attribution requirements.",
        "status": "pending",
        "dependencies": [
          2,
          4,
          8
        ],
        "priority": "medium",
        "details": "Add \"Limitations & Next Steps\" section detailing dataset scope, evaluation caveats, and hardware assumptions. Ensure MIT license present and upstream licenses respected.",
        "testStrategy": "Docs lint (links/spell) passes; LICENSE file exists; README section present and referenced in index.",
        "subtasks": [
          {
            "id": 1,
            "title": "Limitations section",
            "description": "Describe dataset representativeness, hallucination rubric subjectivity, and small-model constraints.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "Section lists at least 3 concrete limitations with rationale."
          },
          {
            "id": 2,
            "title": "Next steps roadmap",
            "description": "Outline immediate enhancements (dataset curation, more evals, CI hardening).",
            "status": "pending",
            "dependencies": [
              1
            ],
            "acceptanceCriteria": "Checklist of 5+ actionable next steps with owners or tags."
          },
          {
            "id": 3,
            "title": "License file",
            "description": "Add MIT LICENSE file and ensure notices for upstream assets as needed.",
            "status": "pending",
            "dependencies": [],
            "acceptanceCriteria": "LICENSE exists; README references license and upstream compliance."
          },
          {
            "id": 4,
            "title": "Attribution audit",
            "description": "Review dependencies for license requirements and add NOTICE if required.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "acceptanceCriteria": "Audit notes committed; no missing attributions detected."
          },
          {
            "id": 5,
            "title": "Doc QA pass",
            "description": "Grammar, links, and command blocks verified; screenshots added if applicable.",
            "status": "pending",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "acceptanceCriteria": "Markdown lints pass and all links resolve."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-24T05:44:42.991Z",
      "updated": "2025-08-24T05:44:42.992Z",
      "description": "Tasks for master context"
    }
  }
}