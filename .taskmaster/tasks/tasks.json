{
  "default": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Project Repository and Environment",
        "description": "Create the basic repository structure, initialize a virtual environment, and create a pinned `requirements.txt` file to ensure a reproducible setup.",
        "details": "Scaffold directories: `src/`, `scripts/`, `configs/`, `eval/`, `tests/`, `api/`, `results/`. Create a virtual environment using `python -m venv .venv`. Install core libraries like `transformers`, `peft`, `bitsandbytes`, `torch`, `datasets`, `fastapi`, `uvicorn`, `pytest` and pin their versions in `requirements.txt`.",
        "testStrategy": "Verify that `pip install -r requirements.txt` runs successfully in a clean environment and all specified directories exist.",
        "priority": "high",
        "dependencies": [],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Project Directories",
            "description": "Create the standard directory structure for the project to house source code, scripts, configurations, and other artifacts.",
            "dependencies": [],
            "details": "Create the following empty directories at the project root: `src/`, `scripts/`, `configs/`, `eval/`, `tests/`, `api/`, `results/`. Add a `.gitkeep` file to each to ensure they can be committed to the repository.",
            "status": "pending",
            "testStrategy": "Verify that all specified directories exist in the project root using `ls -d */`."
          },
          {
            "id": 2,
            "title": "Create Python Virtual Environment",
            "description": "Create an isolated Python virtual environment to manage project dependencies without conflicting with system-wide packages.",
            "dependencies": [],
            "details": "Execute the command `python -m venv .venv` in the project root to create the environment directory.",
            "status": "pending",
            "testStrategy": "Verify the existence of the `.venv/` directory in the project root."
          },
          {
            "id": 3,
            "title": "Initialize Git and Configure .gitignore",
            "description": "Initialize a new Git repository and create a `.gitignore` file to prevent tracking of transient files like the virtual environment.",
            "dependencies": [],
            "details": "Run `git init`. Create a `.gitignore` file that includes entries for the virtual environment (`.venv/`), Python cache (`__pycache__/`), and results (`results/`).",
            "status": "pending",
            "testStrategy": "Confirm that `git status` does not list the `.venv/` directory as an untracked directory."
          },
          {
            "id": 4,
            "title": "Install Core Python Dependencies",
            "description": "Install the required Python libraries for model training, evaluation, and serving into the virtual environment.",
            "dependencies": [],
            "details": "Activate the virtual environment (e.g., `source .venv/bin/activate`). Use pip to install the core libraries: `transformers`, `peft`, `bitsandbytes`, `torch`, `datasets`, `fastapi`, `uvicorn`, `pytest`.",
            "status": "pending",
            "testStrategy": "Run `pip list` within the activated environment and confirm all specified libraries are present."
          },
          {
            "id": 5,
            "title": "Generate Pinned requirements.txt",
            "description": "Create a `requirements.txt` file with the exact versions of all installed dependencies to ensure a reproducible environment.",
            "dependencies": [],
            "details": "With the virtual environment activated, run the command `pip freeze > requirements.txt` to save the list of installed packages and their exact versions to a file in the project root.",
            "status": "pending",
            "testStrategy": "Verify the `requirements.txt` file is created and contains pinned versions for the core libraries. Test by running `pip install -r requirements.txt` in a fresh environment."
          }
        ]
      },
      {
        "id": 2,
        "title": "Define and Validate Data Schema",
        "description": "Define a unified schema for all input data (dialogs, tickets, Q&A) and implement a validator to ensure data quality and consistency.",
        "details": "Implement a Pydantic or similar model for the schema: `{id, inputs:{question, context?}, outputs:{answer}, meta:{source, timestamp, tags[]}}`. The validator should enforce required fields and correct data types.",
        "testStrategy": "Unit test the validator with both valid and invalid data samples to ensure it raises appropriate errors for malformed records.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Core Pydantic Schema Models",
            "description": "Create the basic Pydantic models for the data schema, including nested structures for inputs, outputs, and metadata, based on the specified structure.",
            "dependencies": [],
            "details": "In a new file (e.g., `src/schema.py`), implement Pydantic models: `InputModel` (`question`, `context`), `OutputModel` (`answer`), `MetaModel` (`source`, `timestamp`, `tags`), and a main `DataRecordModel` that composes them along with the top-level `id` field. Use appropriate Python types like `str`, `Optional[str]`, `datetime`, and `List[str]`.",
            "status": "pending",
            "testStrategy": "A code review will verify that all specified fields and their corresponding data types are correctly implemented in the Pydantic models."
          },
          {
            "id": 2,
            "title": "Implement Field-Level Validation Logic",
            "description": "Enhance the Pydantic models with custom validators to enforce constraints beyond basic type checking, such as ensuring required string fields are not empty.",
            "dependencies": [
              "2.1"
            ],
            "details": "Using Pydantic's `@validator` decorator in `src/schema.py`, add logic to ensure that the `id` and `question` fields are non-empty strings. This prevents records with valid types but empty, meaningless values.",
            "status": "pending",
            "testStrategy": "The effectiveness of these validators will be confirmed by the unit tests for invalid data in a subsequent subtask."
          },
          {
            "id": 3,
            "title": "Create a Central Validation Function",
            "description": "Implement a single, reusable function that takes raw data (e.g., a dictionary) and attempts to parse and validate it against the Pydantic schema.",
            "dependencies": [
              "2.2"
            ],
            "details": "Create a function `validate_record(data: dict) -> DataRecordModel` in a new `src/validator.py` file. This function will encapsulate the validation logic by trying to instantiate `DataRecordModel(data)`. It should catch Pydantic's `ValidationError` and can be extended later to log errors.",
            "status": "pending",
            "testStrategy": "This function will be the primary entry point for the unit tests, verifying it correctly processes both valid and invalid data."
          },
          {
            "id": 4,
            "title": "Develop Unit Tests for Valid Data",
            "description": "Create a suite of unit tests to confirm that correctly structured data records pass validation without errors.",
            "dependencies": [
              "2.3"
            ],
            "details": "In a new `tests/test_validator.py` file, use a framework like `pytest` to write test cases. Create sample data dictionaries that conform to the schema, including one with the optional `context` field and one without. Assert that the `validate_record` function successfully returns a `DataRecordModel` instance for each valid sample.",
            "status": "pending",
            "testStrategy": "Run the test suite and ensure all tests for valid data pass, confirming the 'happy path' works as expected."
          },
          {
            "id": 5,
            "title": "Develop Unit Tests for Invalid Data",
            "description": "Add unit tests to ensure the validator correctly identifies and rejects malformed data, raising appropriate errors.",
            "dependencies": [
              "2.3"
            ],
            "details": "In `tests/test_validator.py`, add parameterized tests to cover various failure modes: missing required fields (`id`, `question`), incorrect data types (`id` as an integer), and values that fail custom validation (empty string for `question`). Use `pytest.raises(ValidationError)` to assert that the `validate_record` function raises the expected exception for each invalid case.",
            "status": "pending",
            "testStrategy": "Run the test suite and verify that each test for malformed data correctly triggers a `ValidationError`."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Data Parsers",
        "description": "Create parsers to load raw support data from various formats (e.g., JSON, CSV) into the unified, validated schema.",
        "details": "Develop modular parser functions in `src/data/loaders.py` for each expected input source. Parsers should output a list of objects conforming to the defined schema.",
        "testStrategy": "Unit test each parser with sample raw data files to ensure correct conversion to the target schema.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Unified Data Schema",
            "description": "Establish the standard, unified data structure that all raw data will be converted into. This schema will serve as the single source of truth for a data record.",
            "dependencies": [],
            "details": "In a new file, `src/data/schema.py`, define a Pydantic model or Python dataclass named `UnifiedRecord`. This class will specify the required fields (e.g., `id`, `source_format`, `content`, `metadata`), their data types, and any validation rules.",
            "status": "pending",
            "testStrategy": "The schema's correctness will be validated by the unit tests for the parsers that use it."
          },
          {
            "id": 2,
            "title": "Implement JSON Parser Function",
            "description": "Create a dedicated function to read a JSON file and transform its contents into a list of `UnifiedRecord` objects.",
            "dependencies": [
              "3.1"
            ],
            "details": "In `src/data/loaders.py`, create a function `parse_json(file_path: str) -> List[UnifiedRecord]`. This function will load the JSON data, iterate through the records, and instantiate a `UnifiedRecord` for each, ensuring it conforms to the defined schema.",
            "status": "pending",
            "testStrategy": "A specific unit test will be created in a later subtask to verify this parser with a sample JSON file."
          },
          {
            "id": 3,
            "title": "Implement CSV Parser Function",
            "description": "Create a dedicated function to read a CSV file and transform its rows into a list of `UnifiedRecord` objects.",
            "dependencies": [
              "3.1"
            ],
            "details": "In `src/data/loaders.py`, create a function `parse_csv(file_path: str) -> List[UnifiedRecord]`. Use Python's `csv.DictReader` to process the file row by row, mapping column data to the fields of the `UnifiedRecord` schema.",
            "status": "pending",
            "testStrategy": "A specific unit test will be created in a later subtask to verify this parser with a sample CSV file."
          },
          {
            "id": 4,
            "title": "Create Data Loader Dispatcher Function",
            "description": "Develop a single, high-level loader function that automatically selects the correct parser based on the input file's extension.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "In `src/data/loaders.py`, create a main function `load_data(file_path: str) -> List[UnifiedRecord]`. It will inspect the file extension of `file_path` (e.g., '.json', '.csv') and call the corresponding parser (`parse_json` or `parse_csv`). It should raise a `ValueError` for unsupported file types.",
            "status": "pending",
            "testStrategy": "The dispatcher logic will be tested via unit tests that pass different file types to it."
          },
          {
            "id": 5,
            "title": "Write Unit Tests for All Parsers",
            "description": "Create a comprehensive set of unit tests to validate the JSON parser, CSV parser, and the main dispatcher function.",
            "dependencies": [
              "3.4"
            ],
            "details": "Create a test file, e.g., `tests/test_loaders.py`. Add sample raw data files (`sample.json`, `sample.csv`). Write tests that use `load_data` to parse these files and assert that the output is a list of valid `UnifiedRecord` objects with the expected content. Include a test case for an unsupported file type.",
            "status": "pending",
            "testStrategy": "Run the test suite and confirm that all tests for `src/data/loaders.py` pass successfully."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Dataset Splitting",
        "description": "Create a script to split the unified dataset into training, validation, and test sets to prevent data leakage and enable robust evaluation.",
        "details": "The script must support stratified splitting based on metadata tags (e.g., `source`) to ensure representative distributions. The output should be separate files for each split.",
        "testStrategy": "Verify that the splits are created, their sizes are as expected, and the stratification logic distributes key tags correctly across all splits.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Script Scaffolding and Load Unified Dataset",
            "description": "Set up the basic Python script file (e.g., `scripts/split_dataset.py`) and implement the logic to load the unified dataset from a specified file path.",
            "dependencies": [],
            "details": "The script should use a library like `pandas` or `datasets` to load the unified dataset (e.g., a JSON or CSV file). It should handle basic argument parsing for the input file path.",
            "status": "pending",
            "testStrategy": "Run the script with a path to a sample unified dataset file. Verify that the script executes without errors and successfully loads the data into memory (e.g., by printing the number of loaded records)."
          },
          {
            "id": 2,
            "title": "Implement Basic Random Train/Validation/Test Split",
            "description": "Add core logic to split the loaded dataset into three subsets (training, validation, test) based on specified ratios (e.g., 80-10-10).",
            "dependencies": [
              "4.1"
            ],
            "details": "Use a library function like `sklearn.model_selection.train_test_split` twice to create the three splits. The split ratios should be configurable. For now, a simple random shuffle is sufficient.",
            "status": "pending",
            "testStrategy": "After running the script, check that three distinct data subsets are created in memory. Verify that their combined size equals the original dataset size and that their individual sizes roughly match the target ratios."
          },
          {
            "id": 3,
            "title": "Add Stratification Logic Based on Metadata Tag",
            "description": "Enhance the splitting function to perform stratified sampling based on a specified metadata column (e.g., `source`).",
            "dependencies": [
              "4.2"
            ],
            "details": "Modify the call to `train_test_split` to use the `stratify` parameter. The script should accept a command-line argument to specify which column to stratify on.",
            "status": "pending",
            "testStrategy": "Run the script on a dataset with known `source` distributions. Verify that the proportion of each `source` value is approximately the same across the training, validation, and test sets."
          },
          {
            "id": 4,
            "title": "Implement File Output for Each Split",
            "description": "Add functionality to save the generated training, validation, and test data splits into separate files in a specified output directory.",
            "dependencies": [
              "4.2"
            ],
            "details": "The script should take an output directory path as an argument. It should save the splits as separate files (e.g., `train.jsonl`, `validation.jsonl`, `test.jsonl`) in the specified format.",
            "status": "pending",
            "testStrategy": "Run the script and verify that three files are created in the specified output directory. Check the contents of one file to ensure it has the correct format and data."
          },
          {
            "id": 5,
            "title": "Finalize CLI for Full Configuration",
            "description": "Implement a robust command-line interface (CLI) using a library like `argparse` or `typer` to control all script parameters.",
            "dependencies": [
              "4.3",
              "4.4"
            ],
            "details": "The CLI should allow the user to specify: input file path, output directory, split ratios, the column to stratify on, and a random seed for reproducibility.",
            "status": "pending",
            "testStrategy": "Run the script from the command line with various arguments (e.g., changing ratios, specifying a stratification key). Verify that the script behaves as expected and that the output reflects the provided configuration. Check that using the same random seed produces identical splits."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Data Tokenization",
        "description": "Tokenize the text data from the train/val/test splits using the base model's tokenizer to prepare it for training.",
        "details": "Use the Hugging Face `datasets` library to map a tokenization function over the split datasets. Handle the formatting of inputs and outputs into a single sequence suitable for the model.",
        "testStrategy": "Inspect a few tokenized samples to ensure inputs and outputs are formatted correctly, special tokens are in place, and padding/truncation is applied as expected.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Load Base Model Tokenizer and Raw Datasets",
            "description": "Instantiate the tokenizer from the specified Hugging Face base model and load the pre-processed train, validation, and test dataset splits from disk into a `DatasetDict` object.",
            "dependencies": [],
            "details": "In a new script `src/data/tokenize_data.py`, add logic to load a tokenizer using `AutoTokenizer.from_pretrained()`. Then, use `load_from_disk` to load the datasets prepared by the parsing task.",
            "status": "pending",
            "testStrategy": "Verify that the script successfully loads a tokenizer object and a `DatasetDict` without errors. Print the tokenizer's vocabulary size and the dataset features to confirm."
          },
          {
            "id": 2,
            "title": "Design and Implement Text Formatting Function",
            "description": "Create a Python function that takes a single data example (a dictionary) and formats its input and output fields into a single string, ready for tokenization. This includes adding any required prompt templates or special tokens.",
            "dependencies": [],
            "details": "The function should accept a dictionary representing one row of the dataset and return a single string. For example, it might format it as: `<s>[INST] {question} [/INST] {answer} </s>`. This function will be called by the main tokenization mapper.",
            "status": "pending",
            "testStrategy": "Unit test this function with a sample input dictionary to ensure the output string is formatted exactly as expected, including all special tokens and text fields."
          },
          {
            "id": 3,
            "title": "Implement Batched Tokenization Mapping Function",
            "description": "Develop the core function that will be passed to `dataset.map()`. This function will take a batch of examples, apply the text formatting function from subtask 5.2, and then use the tokenizer to convert the batch of formatted strings into `input_ids`, `attention_mask`, etc.",
            "dependencies": [
              "5.1",
              "5.2"
            ],
            "details": "This function should accept a batch of examples (a dictionary of lists). It will iterate through the batch, apply the formatting, and then call the tokenizer on the list of formatted strings. Set tokenizer parameters like `padding` and `truncation` as needed.",
            "status": "pending",
            "testStrategy": "Create a small, sample batch of data. Pass it to this function along with a loaded tokenizer and verify the output dictionary contains correctly shaped `input_ids` and `attention_mask` tensors."
          },
          {
            "id": 4,
            "title": "Apply Tokenization Across All Dataset Splits",
            "description": "Use the `datasets.map()` method to apply the batched tokenization function to the entire `DatasetDict` (train, validation, and test splits).",
            "dependencies": [
              "5.3"
            ],
            "details": "Call `loaded_datasets.map(tokenization_function, batched=True, num_proc=...)`. Configure `num_proc` to leverage multiple CPU cores for faster processing. Remove original text columns after tokenization to keep the final dataset clean.",
            "status": "pending",
            "testStrategy": "Run the mapping process on a small subset of the data (e.g., `dataset.select(range(100))`). Inspect one of the resulting tokenized samples to ensure `input_ids` and other fields are present and correctly formatted."
          },
          {
            "id": 5,
            "title": "Save Tokenized Datasets to Disk",
            "description": "Save the final, tokenized `DatasetDict` to a dedicated directory on disk using the `save_to_disk` method for efficient loading by the training script.",
            "dependencies": [
              "5.4"
            ],
            "details": "After the `.map()` operation completes, call `tokenized_datasets.save_to_disk('data/processed/tokenized_data')`. This creates a directory with Arrow files and dataset metadata.",
            "status": "pending",
            "testStrategy": "Verify that the target directory and its contents (e.g., `dataset.arrow`, `dataset_info.json`) are created. Write a separate, simple script to test `load_from_disk` on the new directory to ensure it loads without errors."
          }
        ]
      },
      {
        "id": 6,
        "title": "Create Master Data Preparation Script",
        "description": "Combine parsing, validation, splitting, and tokenization into a single, runnable script for an end-to-end data pipeline.",
        "details": "Create `scripts/prepare_data.py` that takes an input directory and an output directory, running the full data ingestion and preprocessing pipeline. It should orchestrate the modules for parsing, validation, and splitting.",
        "testStrategy": "Run the script end-to-end: `python scripts/prepare_data.py --input data/raw --out data/processed`. Verify that the processed and tokenized train/val/test files are created correctly.",
        "priority": "high",
        "dependencies": [
          3,
          4,
          5
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement CLI and Script Skeleton",
            "description": "Create the `scripts/prepare_data.py` file and set up the command-line interface using `argparse` to accept `--input` and `--output` directory arguments. Establish the main orchestration function.",
            "dependencies": [],
            "details": "The script should define a `main` function that parses the command-line arguments. This function will serve as the entrypoint and will orchestrate the calls to subsequent processing functions. Include placeholder function calls for parsing, validation, splitting, and tokenization.",
            "status": "pending",
            "testStrategy": "Run `python scripts/prepare_data.py --help` to verify the arguments are defined. Execute the script with valid paths (`--input data/raw --output data/processed`) to ensure it runs without errors and correctly captures the paths."
          },
          {
            "id": 2,
            "title": "Integrate Data Parsing Logic",
            "description": "Implement the logic to discover raw data files in the input directory, invoke the parsing module on them, and aggregate the results into a single dataset.",
            "dependencies": [
              "6.1"
            ],
            "details": "Within the main script, add a function that takes the input directory path, finds all relevant raw data files (e.g., `.jsonl` files), and uses the existing parsing module (from dependency Task 3) to load and combine them into a unified data structure, such as a Hugging Face `Dataset` or a Pandas DataFrame.",
            "status": "pending",
            "testStrategy": "Add logging to print the number of files found and the total number of records parsed. Run the script and verify that it correctly reads and aggregates data from the specified input directory."
          },
          {
            "id": 3,
            "title": "Integrate Data Validation and Splitting",
            "description": "Orchestrate the calls to the validation and splitting modules. The script will first validate the entire parsed dataset and then split it into training, validation, and test sets.",
            "dependencies": [
              "6.2"
            ],
            "details": "After the parsing step, pass the aggregated dataset to the validation function (from dependency Task 4). If validation succeeds, pass the clean dataset to the splitting function (from dependency Task 5) to create the train/validation/test splits according to the project's defined ratios.",
            "status": "pending",
            "testStrategy": "Log the outcome of the validation step. After splitting, log the number of examples in the train, validation, and test sets to confirm the split was successful."
          },
          {
            "id": 4,
            "title": "Implement Tokenization and Save Processed Splits",
            "description": "Apply tokenization to each data split (train, validation, test) and save the final, processed datasets to the specified output directory.",
            "dependencies": [
              "6.3"
            ],
            "details": "Initialize the appropriate tokenizer from the `transformers` library. Create a tokenization function and apply it to each of the three data splits using `.map()`. Save the resulting tokenized `Dataset` objects to the output directory using a standard format like Arrow or JSON Lines, with clear filenames (e.g., `train.arrow`, `validation.arrow`).",
            "status": "pending",
            "testStrategy": "Execute the script and verify that the tokenized `train`, `validation`, and `test` files are created in the output directory. Load one of the files and inspect its contents to ensure it contains `input_ids` and `attention_mask` columns."
          },
          {
            "id": 5,
            "title": "Add End-to-End Logging and Error Handling",
            "description": "Refine the script with comprehensive logging for each stage and implement robust error handling to ensure pipeline stability and clear reporting.",
            "dependencies": [
              "6.4"
            ],
            "details": "Integrate Python's `logging` module to provide status updates for each major step: parsing, validation, splitting, tokenization, and saving. Wrap key operations in `try...except` blocks to catch potential issues like file not found errors, validation failures, or empty directories, and provide informative error messages to the user.",
            "status": "pending",
            "testStrategy": "Run the full end-to-end script and review the logs for clarity and completeness. Test a failure case, such as providing an invalid input path, and confirm that the script exits gracefully with a descriptive error message."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Core PEFT/LoRA Training Loop",
        "description": "Develop the core training logic to fine-tune a small open LLM using PEFT/LoRA with 4/8-bit quantization to reduce compute costs.",
        "details": "Use `transformers.Trainer` or `trl.SFTTrainer`. Integrate `peft` to create a `LoraConfig` and wrap the model. Use `bitsandbytes` to load the base model in 4/8-bit precision via a `BitsAndBytesConfig`.",
        "testStrategy": "Run a short training loop on a small subset of the data (a 'smoke test') and ensure it completes without errors and a model adapter is successfully saved.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Load Quantized Base Model and Tokenizer",
            "description": "Load the specified pre-trained LLM from the Hugging Face Hub with 4-bit quantization enabled, along with its corresponding tokenizer.",
            "dependencies": [],
            "details": "Instantiate a `BitsAndBytesConfig` for 4-bit NormalFloat (NF4) quantization. Use this config with `AutoModelForCausalLM.from_pretrained` to load the model. Load the tokenizer using `AutoTokenizer.from_pretrained`, ensuring `padding_side='right'` and a pad token are set.",
            "status": "pending",
            "testStrategy": "Verify the loaded model object has the `is_quantized` attribute set to True. Confirm the tokenizer can correctly encode and decode a sample text."
          },
          {
            "id": 2,
            "title": "Configure and Apply PEFT LoRA Adapter",
            "description": "Create a LoRA configuration and apply it to the loaded quantized model to prepare it for parameter-efficient fine-tuning.",
            "dependencies": [
              "7.1"
            ],
            "details": "Instantiate a `LoraConfig` from the `peft` library, specifying parameters like `r`, `lora_alpha`, `lora_dropout`, and `target_modules`. Use `get_peft_model` to wrap the base model with this configuration.",
            "status": "pending",
            "testStrategy": "Print the model architecture after applying PEFT and verify that `LoraLayer` modules have been inserted. Check the number of trainable parameters to confirm it's a small fraction of the total."
          },
          {
            "id": 3,
            "title": "Configure Training Arguments",
            "description": "Define the hyperparameters and settings for the training process using the `transformers.TrainingArguments` class.",
            "dependencies": [],
            "details": "Instantiate `TrainingArguments`, setting essential parameters like `output_dir`, `num_train_epochs`, `per_device_train_batch_size`, `gradient_accumulation_steps`, `optim` (e.g., 'paged_adamw_32bit'), `learning_rate`, `logging_steps`, and `save_strategy`.",
            "status": "pending",
            "testStrategy": "Create the `TrainingArguments` object and print its representation to verify all parameters are set as intended before passing it to the trainer."
          },
          {
            "id": 4,
            "title": "Initialize SFTTrainer",
            "description": "Instantiate the `trl.SFTTrainer` with the prepared model, dataset, tokenizer, and training configurations.",
            "dependencies": [
              "7.2",
              "7.3"
            ],
            "details": "Initialize the `SFTTrainer`, passing the PEFT-wrapped model, training dataset, tokenizer, PEFT config, and `TrainingArguments`. Specify the dataset text field or formatting function if needed.",
            "status": "pending",
            "testStrategy": "Successfully instantiate the `SFTTrainer` object without errors. This validates that all components (model, tokenizer, configs, dataset) are compatible."
          },
          {
            "id": 5,
            "title": "Execute Training Loop (Smoke Test)",
            "description": "Run a short training loop on a small subset of the data to ensure the entire pipeline is functional.",
            "dependencies": [
              "7.4"
            ],
            "details": "Call the `trainer.train()` method. For the initial test, configure `TrainingArguments` with a low `max_steps` (e.g., 10) to run a quick 'smoke test'.",
            "status": "pending",
            "testStrategy": "Execute the training script. Verify that the training loop starts, runs for the specified number of steps without crashing, and the training loss is reported in the logs and shows a decreasing trend."
          },
          {
            "id": 6,
            "title": "Save the Trained LoRA Adapter",
            "description": "After the training loop completes, save the trained LoRA adapter weights to the specified output directory.",
            "dependencies": [
              "7.5"
            ],
            "details": "Call the `trainer.save_model()` method after `trainer.train()` completes. This will save the adapter-specific files (e.g., `adapter_model.bin`, `adapter_config.json`) to the `output_dir`.",
            "status": "pending",
            "testStrategy": "After the script runs, check the specified output directory to confirm that the adapter configuration and model weights files have been created."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement Training Orchestration and Checkpointing",
        "description": "Ensure training runs are reproducible with deterministic seeds and can be resumed from saved checkpoints to handle interruptions.",
        "details": "Set deterministic seeds in PyTorch and other libraries. Configure `TrainingArguments` to save checkpoints at regular intervals. Implement logic to resume training from a specified checkpoint path.",
        "testStrategy": "Start a training run, stop it partway through, and then resume it from the last checkpoint. Verify that the training continues from the correct step by checking logs.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Deterministic Seeding Function",
            "description": "Create and apply a function to set deterministic seeds for PyTorch, NumPy, and Python's random module to ensure run-to-run reproducibility.",
            "dependencies": [],
            "details": "In a utility file (e.g., `src/utils/reproducibility.py`), create a function `set_seed(seed: int)`. This function should set `torch.manual_seed`, `np.random.seed`, and `random.seed`. Also, configure CUDA operations for determinism with `torch.backends.cudnn.deterministic = True` and `torch.backends.cudnn.benchmark = False`. Call this function at the beginning of the main training script.",
            "status": "pending",
            "testStrategy": "Run the training script twice using the same seed for a single step. Verify that the initial model weights and the first batch's loss are identical."
          },
          {
            "id": 2,
            "title": "Configure Periodic Checkpoint Saving in TrainingArguments",
            "description": "Set up the Hugging Face `TrainingArguments` to save model checkpoints periodically based on the number of training steps.",
            "dependencies": [],
            "details": "In the script where `TrainingArguments` is instantiated, configure the following parameters: `save_strategy='steps'`, `save_steps` (e.g., 500), and `save_total_limit` (e.g., 3) to manage disk space by keeping only the most recent checkpoints.",
            "status": "pending",
            "testStrategy": "Start a training run that exceeds the `save_steps` value. Verify that a `checkpoint-<step_number>` directory is created in the specified output directory."
          },
          {
            "id": 3,
            "title": "Implement Logic to Detect and Resume from Checkpoints",
            "description": "Add logic to the training script to automatically detect and resume training from the latest available checkpoint in the output directory.",
            "dependencies": [
              "8.2"
            ],
            "details": "The Hugging Face `Trainer` can handle this automatically. The implementation involves passing `resume_from_checkpoint=True` to the `trainer.train()` call. The script should be structured to allow this call to find existing checkpoints in the `output_dir` defined in `TrainingArguments`.",
            "status": "pending",
            "testStrategy": "Start a training run, stop it after the first checkpoint is saved. Restart the same script without any changes. Verify from the logs that the trainer detects and resumes from the saved checkpoint."
          },
          {
            "id": 4,
            "title": "Add Command-Line Argument for Explicit Checkpoint Resumption",
            "description": "Modify the main training script to accept an optional command-line argument to specify a path to a checkpoint to resume from.",
            "dependencies": [
              "8.3"
            ],
            "details": "Use a library like `argparse` to add a `--resume_from_checkpoint` argument. The value of this argument (a path string) should be passed to the `resume_from_checkpoint` parameter in the `trainer.train()` method. If the argument is not provided, the default behavior (from subtask 8.3) should apply.",
            "status": "pending",
            "testStrategy": "Stop a training run and copy a checkpoint to a new location. Relaunch the training script using the `--resume_from_checkpoint /path/to/new_location` flag and verify it resumes successfully from that specific path."
          },
          {
            "id": 5,
            "title": "Integrate Logging for Training State and Resumption Events",
            "description": "Enhance logging to clearly indicate whether a training run is starting fresh or resuming from a checkpoint, and to track the global step.",
            "dependencies": [
              "8.3"
            ],
            "details": "Before initiating the `Trainer`, add log statements that check if a resume operation is about to occur. Log a clear message, such as 'Starting new training run.' or 'Resuming training from checkpoint: {path}'. Ensure the standard training logs (e.g., from `WandbCallback` or `DefaultFlowCallback`) correctly display the global step continuing from the checkpoint's state.",
            "status": "pending",
            "testStrategy": "Run a resumed training job. Check the console output or log file (e.g., `wandb` run page) to confirm that the 'Resuming training...' message is present and that the logged training steps (e.g., 'Step 500/2000') continue from the correct number."
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement Configuration Management for Training",
        "description": "Allow hyperparameters and run settings to be controlled via configuration files and CLI arguments for flexible experimentation.",
        "details": "Use a library like `PyYAML` and `argparse` to load settings from a YAML file (e.g., `configs/sft.yaml`) and allow overrides from the command line. Expose key hyperparameters like learning rate, LoRA rank/alpha, and epochs.",
        "testStrategy": "Run training with a config file. Then, run it again with a CLI argument overriding a value from the file. Verify the correct configuration is used in both runs by checking the logs.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Configuration Schema using Data Classes",
            "description": "Establish a clear, typed structure for all configuration parameters using Python data classes. This schema will act as the single source of truth for settings related to the model, data, and training process.",
            "dependencies": [],
            "details": "Create a new module, e.g., `src/config/schema.py`. Define nested data classes for logical groups like `ModelConfig`, `DataConfig`, and `TrainingConfig`. Include fields for key parameters such as `learning_rate`, `num_train_epochs`, `lora_r`, `lora_alpha`, model paths, and data paths.",
            "status": "pending",
            "testStrategy": "This is a structural task. It will be implicitly validated by the successful implementation of the loading and parsing subtasks that depend on it."
          },
          {
            "id": 2,
            "title": "Create Default YAML Configuration File",
            "description": "Create a baseline YAML configuration file (`configs/sft.yaml`) that contains sensible default values for all parameters defined in the configuration schema.",
            "dependencies": [
              "9.1"
            ],
            "details": "The YAML structure must directly map to the data classes defined in `src/config/schema.py`. Populate this file with initial, working values for all required parameters, ensuring the training script can run successfully using only this file.",
            "status": "pending",
            "testStrategy": "Manually review the YAML file to ensure it is well-formed, syntactically correct, and its structure perfectly matches the schema from subtask 9.1."
          },
          {
            "id": 3,
            "title": "Implement YAML File Loading Logic",
            "description": "Develop a function to parse the YAML configuration file from a given path and load its contents into the corresponding Python data class objects defined in the schema.",
            "dependencies": [
              "9.1",
              "9.2"
            ],
            "details": "In a new module like `src/config/loader.py`, create a function `load_config_from_yaml(path)`. Use the `PyYAML` library to read and parse the file. The function should return an instance of the main configuration data class, populated with values from the file. Implement error handling for file-not-found scenarios.",
            "status": "pending",
            "testStrategy": "Write a unit test that loads the default `configs/sft.yaml` and asserts that the resulting Python object has the correct types and values for several key parameters (e.g., learning rate, LoRA rank)."
          },
          {
            "id": 4,
            "title": "Implement CLI Argument Parser for Overrides",
            "description": "Integrate `argparse` to parse command-line arguments and use them to override the settings loaded from the YAML configuration file.",
            "dependencies": [
              "9.3"
            ],
            "details": "In the main training script or a helper function, use `argparse` to define arguments that mirror the configuration schema. A good practice is to use dot notation for nested parameters (e.g., `--training.learning_rate`). The logic should first load the config from YAML, then iterate through parsed CLI arguments, updating the configuration object with any provided values.",
            "status": "pending",
            "testStrategy": "Run the training script with a CLI argument like `--learning_rate 0.0005`. Verify by checking the application logs that the overridden value is used by the trainer, not the default value from the YAML file."
          },
          {
            "id": 5,
            "title": "Integrate Unified Config into Training Pipeline",
            "description": "Refactor the main training script to use the loaded and merged configuration object as the sole source for setting up all training components.",
            "dependencies": [
              "9.4"
            ],
            "details": "Modify the `main` function of the training script. At the start, call the logic to load the YAML and apply CLI overrides. Pass the resulting config object to all functions that initialize the model, tokenizer, data loaders, and Hugging Face `TrainingArguments`. Remove all hardcoded hyperparameters and replace them with attributes from the config object (e.g., `training_args.learning_rate = config.training.learning_rate`).",
            "status": "pending",
            "testStrategy": "Execute a full training run using only a configuration file. Confirm that the run completes successfully and that the logs (e.g., from the Hugging Face Trainer) show that the hyperparameters used match the values specified in the `configs/sft.yaml` file."
          }
        ]
      },
      {
        "id": 10,
        "title": "Create Master Training Script",
        "description": "Create the main entrypoint script for launching training runs, tying together data, model, and configuration.",
        "details": "Create `scripts/train.py` that accepts a configuration file (e.g., `--config configs/sft.yaml`). This script will initialize the model, data, and trainer, and start the fine-tuning process.",
        "testStrategy": "Execute `python scripts/train.py --config configs/sft.yaml` and confirm a full training run completes, saving checkpoints and the final model adapter to the specified output directory.",
        "priority": "high",
        "dependencies": [
          6,
          8,
          9
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Argument Parsing and Configuration Loading",
            "description": "Create the main function in `scripts/train.py` that uses `argparse` to accept a `--config` command-line argument. Load the specified YAML file and parse it into a configuration object or dictionary.",
            "dependencies": [],
            "details": "Use the `argparse` library for command-line argument handling and `PyYAML` to load the configuration file. The script should fail gracefully if the config file is not found or is invalid. The loaded configuration will be used by all subsequent subtasks.",
            "status": "pending",
            "testStrategy": "Run `python scripts/train.py --config configs/sft.yaml` and print the loaded configuration dictionary to the console to verify it's parsed correctly."
          },
          {
            "id": 2,
            "title": "Initialize Model and Tokenizer",
            "description": "Using the loaded configuration, initialize the tokenizer and the base model from Hugging Face. This includes applying any specified quantization or PEFT (LoRA) configurations.",
            "dependencies": [
              "10.1"
            ],
            "details": "Use `transformers.AutoTokenizer.from_pretrained` and `transformers.AutoModelForCausalLM.from_pretrained`. The model name, quantization settings (e.g., `load_in_4bit`), and LoRA config (using `peft.get_peft_model`) should be read from the configuration object.",
            "status": "pending",
            "testStrategy": "Add code to load the model and tokenizer, then print the model's architecture to confirm successful initialization and application of LoRA adapters."
          },
          {
            "id": 3,
            "title": "Load and Prepare Datasets",
            "description": "Load the pre-tokenized training and validation datasets from the paths specified in the configuration file.",
            "dependencies": [
              "10.1"
            ],
            "details": "Use the `datasets` library to load the Arrow-formatted datasets created by the data preparation script (Task 6). The paths for `train_dataset` and `eval_dataset` will be provided in the config.",
            "status": "pending",
            "testStrategy": "After loading, print the number of examples in the training and validation sets (e.g., `len(train_dataset)`) to confirm they are loaded correctly."
          },
          {
            "id": 4,
            "title": "Configure and Initialize Trainer",
            "description": "Instantiate the `transformers.Trainer` class, providing it with the initialized model, datasets, and training arguments. All training hyperparameters should be sourced from the configuration file.",
            "dependencies": [
              "10.2",
              "10.3"
            ],
            "details": "Create a `transformers.TrainingArguments` object, populating it with parameters from the config like `output_dir`, `per_device_train_batch_size`, `learning_rate`, `num_train_epochs`, `logging_steps`, and `evaluation_strategy`. Pass this, along with the model and datasets, to the `Trainer` constructor.",
            "status": "pending",
            "testStrategy": "Successfully instantiate the `Trainer` object without runtime errors. Log the `TrainingArguments` to verify they match the config file."
          },
          {
            "id": 5,
            "title": "Launch Training and Save Artifacts",
            "description": "Execute the training run by calling the `train()` method on the trainer instance. After training is complete, save the final model adapter to the specified output directory.",
            "dependencies": [
              "10.4"
            ],
            "details": "Call `trainer.train()`. Upon completion, call `trainer.save_model()` to ensure the final adapter, configuration, and tokenizer files are written to the `output_dir` specified in the training arguments.",
            "status": "pending",
            "testStrategy": "Execute the full script with a small dataset or for a few steps. Verify that the training process starts, logs are produced, and the final adapter files (e.g., `adapter_model.safetensors`) are saved in the correct output directory."
          }
        ]
      },
      {
        "id": 11,
        "title": "Develop Evaluation Question Suite",
        "description": "Create the initial set of 100 evaluation questions for measuring model performance, helpfulness, and hallucination rate.",
        "details": "Create a JSON file, `eval/suites/support_100.json`, containing 100 questions. These questions should be representative of real user queries. Include ground truth answers where possible for rubric-based scoring.",
        "testStrategy": "Review the question set with a support lead or analyst to ensure it covers a diverse range of topics and difficulties relevant to the tech-support domain.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Evaluation Categories and Question Distribution",
            "description": "Identify and define the key topics for the evaluation questions to ensure comprehensive coverage of real user queries. Determine the number of questions to be created for each category.",
            "dependencies": [],
            "details": "Analyze sources like past support tickets, product documentation, and user forums to identify 5-10 core categories (e.g., Account Management, Billing, API Integration, Product Feature X). Create a plan allocating the 100 questions across these categories to ensure a balanced and representative set.",
            "status": "pending",
            "testStrategy": "Review the defined categories and distribution plan with a support lead to confirm they are representative of common user issues."
          },
          {
            "id": 2,
            "title": "Draft 100 Raw Evaluation Questions",
            "description": "Write the initial text for 100 evaluation questions based on the categories and distribution plan defined in the previous subtask.",
            "dependencies": [
              "11.1"
            ],
            "details": "For each category, create questions that vary in difficulty and phrasing to mimic real user queries. The focus is on generating realistic prompts. The output at this stage is a simple list or document of the questions themselves, ready for annotation.",
            "status": "pending",
            "testStrategy": "A quick peer review to check for clarity, realism, and diversity in the drafted questions."
          },
          {
            "id": 3,
            "title": "Author Ground Truth Answers and Assign Metadata",
            "description": "For each of the 100 drafted questions, write a definitive 'ground truth' answer and assign relevant metadata.",
            "dependencies": [
              "11.2"
            ],
            "details": "Review each question and write the ideal, correct answer that will serve as the benchmark for rubric-based scoring. Assign the corresponding category tag and a unique ID to each question-answer pair. This step is critical for measuring accuracy and hallucination.",
            "status": "pending",
            "testStrategy": "Spot-check 10-15 ground truth answers for factual accuracy and completeness. Ensure all questions have been assigned a category."
          },
          {
            "id": 4,
            "title": "Assemble and Validate the Final JSON Evaluation File",
            "description": "Compile the questions, ground truth answers, and metadata into the final JSON file and conduct a final review with stakeholders.",
            "dependencies": [
              "11.3"
            ],
            "details": "Create the `eval/suites/support_100.json` file. Structure it as a JSON array where each object contains fields like `id`, `category`, `prompt` (the question), and `ground_truth` (the answer). Validate the JSON syntax to ensure it is machine-readable.",
            "status": "pending",
            "testStrategy": "Perform the full review of the complete question set with a support lead as specified in the parent task. Validate the file by parsing it with a script to ensure it's well-formed."
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement Automated Evaluation Script",
        "description": "Create a script to run a fine-tuned model and a base model against the evaluation suite to generate comparable outputs.",
        "details": "Create `scripts/eval.py`. It should load a fine-tuned model (adapter + base) and a base model, run inference for every question in the suite for both models, and save the generated answers.",
        "testStrategy": "Run the script with a trained model and a base model. Verify that it generates outputs for all 100 questions for both models and saves them to a structured file.",
        "priority": "high",
        "dependencies": [
          10,
          11
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Implement Win-Rate Comparison",
        "description": "Add logic to the evaluation script to perform a side-by-side win-rate test between the fine-tuned model and the base model.",
        "details": "The evaluation script should produce a structured output (e.g., CSV or JSON) with columns for the question, base model answer, and fine-tuned model answer, to facilitate side-by-side review and scoring.",
        "testStrategy": "Generate a side-by-side comparison file. Manually review a sample of 10 comparisons to ensure the format is clear and the correct model outputs are being compared.",
        "priority": "high",
        "dependencies": [
          12
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Metrics Reporting",
        "description": "Calculate and report key metrics like win rate, hallucination rate, and training cost/time to provide clear success criteria.",
        "details": "Aggregate the results from the evaluation script. Implement a stub for a hallucination rubric. Print a summary table to the console and write a detailed CSV/JSON report to the `results/` directory.",
        "testStrategy": "Run the full eval pipeline and check that a summary table is printed to the console and a results file (e.g., `results/eval_summary.json`) is created with the correct structure and metrics.",
        "priority": "high",
        "dependencies": [
          13
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Create CLI Demo for Interactive Q&A",
        "description": "Provide a simple command-line interface for users to quickly validate and interact with the fine-tuned model.",
        "details": "Create `demo.py` that loads the fine-tuned model and enters a read-eval-print loop, accepting user input and printing the model's generation. Ensure it can be run in under 60 seconds from a fresh checkout.",
        "testStrategy": "Run `python demo.py`, ask a question, and verify that a coherent answer is generated from the loaded model.",
        "priority": "high",
        "dependencies": [
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Develop FastAPI Inference Endpoint",
        "description": "Create a web service for programmatic access to the model's generation capabilities, enabling integration with other tools.",
        "details": "In the `api/` directory, create a FastAPI application with a `POST /generate` endpoint that accepts a prompt and returns a generation, and a `GET /healthz` endpoint that returns a status.",
        "testStrategy": "Run `uvicorn api.app:app`. Send a `GET` request to `/healthz` and verify a 200 OK response. Send a `POST` request to `/generate` with a prompt and verify a JSON response with the generated text is returned.",
        "priority": "high",
        "dependencies": [
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Implement Model Packaging Utility",
        "description": "Create a utility to save the final model artifacts (adapter, config, tokenizer) for consistent deployment and sharing.",
        "details": "The utility should save the trained LoRA adapter weights, the adapter configuration, and the tokenizer files in the standard Hugging Face format. Create a simple loader utility to demonstrate loading these artifacts for inference.",
        "testStrategy": "Package a trained model. Use the loader utility in a separate script to load the packaged artifacts and run a sample inference to ensure it works correctly.",
        "priority": "high",
        "dependencies": [
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Write Unit Tests for Data Pipeline",
        "description": "Create unit tests for the data loading, validation, and splitting components to guard against regressions.",
        "details": "Using `pytest`, write tests in the `tests/` directory for the schema validator, data parsers, and splitting logic. Use small, self-contained data fixtures.",
        "testStrategy": "Run `pytest tests/test_data.py` and ensure all tests pass.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Write Smoke Tests for Training and Inference",
        "description": "Create fast-running smoke tests for the main training and inference workflows to ensure core functionality remains intact.",
        "details": "In `tests/`, create a test that runs `scripts/train.py` for a single step on a tiny dataset. Create another test that loads a mock model and tests the CLI demo and FastAPI endpoint for basic functionality.",
        "testStrategy": "Run `pytest tests/test_smoke.py` and ensure the tests complete quickly and successfully.",
        "priority": "high",
        "dependencies": [
          10,
          15,
          16
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Setup Basic CI Checks",
        "description": "Integrate the tests and style checks into a Continuous Integration pipeline to automate quality control.",
        "details": "Create a GitHub Actions or similar CI configuration file that triggers on push/pull_request. The CI job should install dependencies, run `pytest`, and run style checkers like `black` or `ruff`.",
        "testStrategy": "Make a small change and push it to a branch. Verify that the CI pipeline triggers and all checks pass successfully.",
        "priority": "high",
        "dependencies": [
          18,
          19
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Add Configurable Training Recipes (SFT/DPO)",
        "description": "Extend the training script to support different training objectives, such as Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).",
        "details": "Refactor the training script to allow selection of a trainer (e.g., `SFTTrainer` or `DPOTrainer` from TRL) based on a configuration setting. This may require adjustments to the data preparation pipeline to support preference data for DPO.",
        "testStrategy": "Create a config for an SFT recipe and a DPO recipe (with mock preference data). Run a smoke test for each to ensure the correct trainer is initialized and runs without error.",
        "priority": "medium",
        "dependencies": [
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Implement Best Model Selection",
        "description": "Automatically save the best model checkpoint based on a specified evaluation metric to improve model quality.",
        "details": "Configure `TrainingArguments` with `load_best_model_at_end=True` and `metric_for_best_model`. The evaluation loop must be integrated into the training process to compute the metric on the validation set periodically.",
        "testStrategy": "Run a training job with evaluation enabled. Verify that the logs show the evaluation metric being tracked and that the final saved model corresponds to the checkpoint with the best score.",
        "priority": "medium",
        "dependencies": [
          14
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Implement Robust Input Length Handling",
        "description": "Add strategies for handling inputs that exceed the model's maximum sequence length, such as truncation or sliding window.",
        "details": "Implement a simple truncation strategy as the default. Add an optional sliding-window strategy for handling long contexts, configurable via the data preparation script.",
        "testStrategy": "Create a test case with a document longer than the max sequence length. Verify that the truncation strategy shortens it correctly. If implementing sliding window, verify it creates multiple overlapping chunks as expected.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Enhance Logging and Error Handling",
        "description": "Improve logging throughout the application for better traceability and add graceful failure paths for robustness.",
        "details": "Use Python's `logging` module to provide clear, structured logs with run IDs and output paths. Add `try...except` blocks around critical I/O and compute operations to provide informative error messages.",
        "testStrategy": "Manually inspect logs from a full run to ensure clarity. Intentionally cause an error (e.g., point to a missing file) and verify that the application exits gracefully with a helpful message.",
        "priority": "medium",
        "dependencies": [
          10,
          14,
          16
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Add Mixed-Precision and Gradient Accumulation Presets",
        "description": "Provide configuration presets for common optimization techniques to reduce memory usage and effectively increase batch size.",
        "details": "Add configuration options in the YAML file for `fp16`/`bf16` mixed-precision training and `gradient_accumulation_steps`. Provide comments in the default config file explaining their use cases.",
        "testStrategy": "Run a smoke test with `fp16=True` and `gradient_accumulation_steps=2`. Verify from the logs that these settings are active and the run completes successfully.",
        "priority": "low",
        "dependencies": [
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Implement Hyperparameter Sweeps and Early Stopping",
        "description": "Enable automated hyperparameter tuning and early stopping to find optimal settings and prevent overfitting.",
        "details": "Integrate with a library like Weights & Biases Sweeps or Optuna to run automated sweeps for parameters like LoRA rank/alpha. Implement early stopping by configuring `EarlyStoppingCallback` in the trainer.",
        "testStrategy": "Configure and run a small sweep over 2-3 trials. Verify that the runs are logged and a best model is identified. Configure early stopping and verify it stops training prematurely when the validation metric plateaus.",
        "priority": "low",
        "dependencies": [
          22
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 27,
        "title": "Create Lightweight Model Registry",
        "description": "Create a simple file-based model registry to track trained models and their associated metadata and performance.",
        "details": "When a model is packaged, also write a metadata JSON file to a `registry/` directory. The metadata should include the run ID, base model, performance metrics, and path to the artifacts.",
        "testStrategy": "Run a training and packaging job. Verify that a corresponding metadata file is created in the `registry/` directory with the correct information.",
        "priority": "low",
        "dependencies": [
          17
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 28,
        "title": "Implement Prompt Templating System",
        "description": "Allow users to define and select different prompt templates for inference to easily experiment with prompting strategies.",
        "details": "Create a system where prompt templates (e.g., using f-strings or Jinja2) can be stored in a configuration file. The inference code (CLI, FastAPI) should be able to load a named template and format the user's input.",
        "testStrategy": "Define two different prompt templates. Test the demo and API by specifying each template and verifying that the final prompt sent to the model is formatted correctly.",
        "priority": "low",
        "dependencies": [
          15,
          16
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 29,
        "title": "Add Optional Guardrail Hooks",
        "description": "Implement a mechanism to check model outputs against safety and topical guardrails to prevent undesirable responses.",
        "details": "In the inference pipeline, add a configurable post-processing step that can apply checks to the generated text. This could include regex/keyword matching for unsafe content or out-of-scope topics.",
        "testStrategy": "Define a simple guardrail (e.g., block a specific keyword). Send a prompt that is likely to elicit that keyword and verify the output is blocked or replaced with a canned response.",
        "priority": "low",
        "dependencies": [
          15,
          16
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Create Dockerfile for Packaging",
        "description": "Create a Dockerfile to package the FastAPI application and its dependencies for portable and scalable deployment.",
        "details": "The Dockerfile should start from a Python base image, copy the `requirements.txt` and source code, install dependencies, and define the `CMD` to run the `uvicorn` server. It should also handle downloading a default model.",
        "testStrategy": "Build the Docker image using `docker build`. Run the container and test the `/healthz` and `/generate` endpoints from the host machine to ensure the service is running correctly inside the container.",
        "priority": "low",
        "dependencies": [
          16,
          17
        ],
        "status": "todo",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-25T19:15:46.883Z",
      "updated": "2025-08-25T19:15:46.883Z",
      "description": "Tasks for default context"
    }
  }
}