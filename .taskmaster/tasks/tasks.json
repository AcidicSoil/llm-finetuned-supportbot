{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Finalize Core Technical Decisions",
        "description": "Address all 'Open Questions' from the PRD to establish a clear technical baseline for the project. This is a prerequisite for most implementation tasks.",
        "details": "Decide on: 1) The specific small open LLM(s) to use. 2) The tech-support dataset and any preprocessing steps. 3) The target hardware assumptions (GPU/memory). 4) The detailed evaluation procedure, including the manual rubric for hallucinations. 5) The initial scope for CI checks.",
        "testStrategy": "Review a written decision log or updated PRD section confirming that all open questions have been answered and documented for the team.",
        "priority": "high",
        "dependencies": [],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Select and Configure Base LLM and Quantization",
            "description": "Choose the specific open-source LLM checkpoint and define the 4-bit quantization settings to establish the model baseline.",
            "dependencies": [],
            "details": "Create a configuration file `configs/model_config.yaml`. Specify the `model_name_or_path` (e.g., 'mistralai/Mistral-7B-Instruct-v0.2') and the 4-bit quantization settings (e.g., `load_in_4bit: true`, `bnb_4bit_quant_type: 'nf4'`, `bnb_4bit_compute_dtype: 'bfloat16'`).",
            "status": "pending",
            "testStrategy": "Review `configs/model_config.yaml` to confirm it contains the chosen model checkpoint and complete quantization settings."
          },
          {
            "id": 2,
            "title": "Define Dataset Path and Prompt Template",
            "description": "Finalize the tech-support dataset to be used for fine-tuning and document the exact prompt template for training and inference.",
            "dependencies": [],
            "details": "Create `configs/data_config.yaml` specifying the `dataset_path` and format. Create a new document `docs/prompt_engineering.md` to define the precise prompt template, including roles, instructions, and placeholders.",
            "status": "pending",
            "testStrategy": "Verify `configs/data_config.yaml` contains the dataset path. Review `docs/prompt_engineering.md` to ensure the prompt template is clearly defined."
          },
          {
            "id": 3,
            "title": "Configure Default PEFT/LoRA Parameters",
            "description": "Establish the default Parameter-Efficient Fine-Tuning (PEFT) configuration for LoRA to be used in initial training runs.",
            "dependencies": [],
            "details": "Create `configs/peft_config.yaml`. Define the initial LoRA parameters: `r` (e.g., 16), `lora_alpha` (e.g., 32), `lora_dropout` (e.g., 0.05), and `target_modules` (e.g., `['q_proj', 'v_proj']`).",
            "status": "pending",
            "testStrategy": "Review `configs/peft_config.yaml` to ensure all required LoRA parameters (`r`, `lora_alpha`, `lora_dropout`, `target_modules`) are present with default values."
          },
          {
            "id": 4,
            "title": "Set Initial Training Hyperparameters for Demo Run",
            "description": "Define a complete set of training hyperparameters for a small-scale, initial demonstration run to validate the training pipeline.",
            "dependencies": [],
            "details": "Create `configs/training_args.yaml`. Specify key hyperparameters like `learning_rate` (e.g., 2e-4), `per_device_train_batch_size` (e.g., 1), `num_train_epochs` (e.g., 1), `weight_decay` (e.g., 0.01), and `gradient_accumulation_steps` (e.g., 4).",
            "status": "pending",
            "testStrategy": "Check `configs/training_args.yaml` for the presence and sensible default values for all specified training hyperparameters."
          },
          {
            "id": 5,
            "title": "Document Manual Evaluation Rubric and Procedure",
            "description": "Create a detailed manual evaluation rubric, with a focus on identifying hallucinations, and document the overall evaluation procedure.",
            "dependencies": [],
            "details": "Create a new file `docs/evaluation_procedure.md`. Define a structured rubric for manual evaluation (e.g., scales for helpfulness/correctness, a binary flag/reason for hallucinations). Outline the step-by-step process for sampling and evaluating model outputs.",
            "status": "pending",
            "testStrategy": "Review `docs/evaluation_procedure.md` to confirm it contains a detailed rubric with clear criteria and a step-by-step evaluation process."
          },
          {
            "id": 6,
            "title": "Specify Target Hardware and Initial CI Scope",
            "description": "Document the assumed target hardware for development and training, and define the initial scope for Continuous Integration (CI) checks.",
            "dependencies": [],
            "details": "Create `docs/project_setup.md`. Add a 'Hardware Assumptions' section detailing the target GPU and memory (e.g., 'NVIDIA L4 24GB' or 'NVIDIA A100 40GB'). Add an 'Initial CI Scope' section outlining the first checks (e.g., 'Run pytest', 'Run ruff linter').",
            "status": "pending",
            "testStrategy": "Review `docs/project_setup.md` to ensure the 'Hardware Assumptions' and 'Initial CI Scope' sections are present and contain specific information."
          }
        ]
      },
      {
        "id": 2,
        "title": "Scaffold Project Repository Structure",
        "description": "Create the initial directory structure and placeholder files as specified in the PRD to establish the project's foundation.",
        "details": "Fulfills FR-7. The repository must contain the following top-level items: `src/`, `demo.py`, `eval/`, `results/`, `tests/`, `requirements.txt`, and `README.md`.",
        "testStrategy": "Clone the repository and verify that all specified directories and files exist in the main branch.",
        "priority": "high",
        "dependencies": [],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Core Project Directories",
            "description": "Establish the foundational directory structure for the project by creating all necessary top-level and nested directories as specified in the PRD.",
            "dependencies": [],
            "details": "Create the following empty directories at the root of the repository: `src/`, `eval/`, `results/`, `tests/`, `docs/`, and `.github/workflows/`. The `results/` directory is for outputs and should be created but remain empty.",
            "status": "pending",
            "testStrategy": "Acceptance criteria: Run `ls -R` or an equivalent command in the project root to verify that all specified directories (`src/`, `eval/`, `results/`, `tests/`, `docs/`, `.github/workflows/`) exist."
          },
          {
            "id": 2,
            "title": "Add Source Code Placeholder Files",
            "description": "Create minimal, runnable placeholder Python files in the `src/` directory to define the structure for the core training, data, and model logic.",
            "dependencies": [],
            "details": "In the `src/` directory, create: `__init__.py` (can be empty), `train.py`, `data.py`, and `model.py`. Each `.py` file (except `__init__.py`) should contain a minimal function (e.g., `def main(): print('Module executed')`) and an `if __name__ == '__main__':` block to ensure it can be executed without errors.",
            "status": "pending",
            "testStrategy": "Acceptance criteria: Execute `python src/train.py`, `python src/data.py`, and `python src/model.py` from the command line. Each script must run to completion without raising an exception."
          },
          {
            "id": 3,
            "title": "Add Application, Demo, and Test Script Stubs",
            "description": "Create placeholder scripts for the FastAPI application, command-line demo, evaluation entry point, and a basic smoke test.",
            "dependencies": [],
            "details": "Create the following files with minimal, runnable content: `demo.py` (prints a message), `app.py` (a minimal FastAPI app with a root endpoint returning `{\"status\": \"ok\"}`), `eval/run_eval.py` (prints a message), and `tests/test_smoke.py` (contains a single test `def test_always_passes(): assert True`).",
            "status": "pending",
            "testStrategy": "Acceptance criteria: `python demo.py` and `python eval/run_eval.py` run without error. `pytest tests/test_smoke.py` runs and passes. The FastAPI app can be started and a `curl` to its root endpoint returns a successful JSON response."
          },
          {
            "id": 4,
            "title": "Initialize Dependencies and Git Ignore",
            "description": "Define the initial project dependencies in `requirements.txt` and configure the `.gitignore` file to exclude unnecessary files from version control.",
            "dependencies": [],
            "details": "Create `requirements.txt` and add minimal dependencies for the stubs: `fastapi`, `uvicorn[standard]`, and `pytest`. Create a `.gitignore` file and add common Python/IDE patterns (`__pycache__/`, `.venv/`, `*.pyc`) and project-specific directories (`results/`).",
            "status": "pending",
            "testStrategy": "Acceptance criteria: After creating a virtual environment, `pip install -r requirements.txt` completes successfully. `git status` does not show any files or directories listed in `.gitignore`."
          },
          {
            "id": 5,
            "title": "Scaffold README.md with Key Sections",
            "description": "Create the main `README.md` file with a project title and placeholder sections for essential project documentation.",
            "dependencies": [],
            "details": "Create a `README.md` file at the project root. It must include a project title (H1) and the following H2-level markdown sections: `## Quickstart`, `## Evaluation`, and `## Demos`. Each section should contain a placeholder sentence, e.g., 'Details to be added.'",
            "status": "pending",
            "testStrategy": "Acceptance criteria: Open the `README.md` file in a viewer and confirm the presence and correct markdown formatting of the title and all three specified sections."
          },
          {
            "id": 6,
            "title": "Add Placeholder GitHub Actions CI Workflow",
            "description": "Create a basic CI workflow file to ensure the repository is set up for continuous integration and that the smoke test runs automatically.",
            "dependencies": [],
            "details": "Create the file `.github/workflows/ci.yml`. The workflow should trigger on push to the `main` branch. It must define a job that checks out the code, sets up Python, runs `pip install -r requirements.txt`, and executes `pytest tests/test_smoke.py`.",
            "status": "pending",
            "testStrategy": "Acceptance criteria: After pushing the initial commits to a GitHub repository, the CI workflow is triggered automatically and completes successfully in the 'Actions' tab."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Quickstart Environment Setup",
        "description": "Populate the `requirements.txt` file and document the virtual environment setup process in the README to ensure users can easily prepare their environment.",
        "details": "Fulfills FR-1 and NFR-2. The `requirements.txt` must include all necessary dependencies (e.g., PyTorch, Transformers, PEFT, BitsAndBytes). The README must contain the commands to create a venv, activate it, and install dependencies.",
        "testStrategy": "On a clean machine or in a fresh environment, execute the setup commands from the README. The process must complete without errors.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Identify Core Dependencies",
            "description": "Research and finalize the list of Python packages required for the project, including specific versions for reproducibility.",
            "dependencies": [],
            "details": "Deliverable: A finalized list of packages (e.g., `torch`, `transformers`, `peft`, `bitsandbytes`, `accelerate`). Acceptance Criteria: The list is complete and versions are chosen to ensure compatibility.",
            "status": "pending",
            "testStrategy": "Validated by the successful installation in subtask 3.2. No direct command."
          },
          {
            "id": 2,
            "title": "Create and Populate requirements.txt",
            "description": "Create the `requirements.txt` file in the project root and add all dependencies identified in the previous step.",
            "dependencies": [
              "3.1"
            ],
            "details": "Deliverable: `requirements.txt`. Acceptance Criteria: The file exists at the project root and contains all necessary packages with pinned versions.",
            "status": "pending",
            "testStrategy": "In a new virtual environment, run: `pip install -r requirements.txt`. The command must complete without any installation errors."
          },
          {
            "id": 3,
            "title": "Draft 'Quickstart' Section in README",
            "description": "Add a new top-level 'Quickstart' or 'Environment Setup' section to the `README.md` file to house the setup instructions.",
            "dependencies": [],
            "details": "Deliverable: `README.md`. Acceptance Criteria: The `README.md` file contains a new, clearly marked markdown section (e.g., `## Quickstart`).",
            "status": "pending",
            "testStrategy": "Visually inspect the rendered `README.md` to confirm the new section exists."
          },
          {
            "id": 4,
            "title": "Add venv Commands to README",
            "description": "Document the commands for creating and activating a Python virtual environment in the 'Quickstart' section of the README.",
            "dependencies": [
              "3.3"
            ],
            "details": "Deliverable: `README.md`. Acceptance Criteria: The README provides commands for venv creation (e.g., `python3 -m venv venv`) and activation for both Linux/macOS (`source venv/bin/activate`) and Windows (`venv\\Scripts\\activate`).",
            "status": "pending",
            "testStrategy": "On a clean system, copy and paste the commands from the README into a terminal to verify they create and activate a virtual environment successfully."
          },
          {
            "id": 5,
            "title": "Add Install Command and Validate",
            "description": "Add the final `pip install` command to the README and perform a full end-to-end validation of the entire documented setup process.",
            "dependencies": [
              "3.2",
              "3.4"
            ],
            "details": "Deliverable: `README.md`. Acceptance Criteria: The README includes the `pip install -r requirements.txt` command. The full sequence of commands in the README runs successfully from start to finish.",
            "status": "pending",
            "testStrategy": "On a clean machine/container, execute the full sequence of commands from the README. Example for Linux: `python3 -m venv venv && source venv/bin/activate && pip install -r requirements.txt`. The process must complete without errors."
          }
        ]
      },
      {
        "id": 4,
        "title": "Define and Document Baseline & Target Metrics",
        "description": "Formally define and document the TBD baseline and target values for all success metrics to provide clear goals for evaluation.",
        "details": "Fulfills FR-8. The README or another results document must be updated to specify the target values for 'Win-rate vs. base', 'Hallucination rate', and 'Training cost & time'.",
        "testStrategy": "Review the project's README to confirm that the 'Success Metrics' or 'Roadmap' section contains specific, non-TBD values for baselines and targets.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Metric Definitions to README",
            "description": "Create a 'Success Metrics' section in the README and write clear, formal definitions for 'Win-rate vs. base', 'Hallucination rate', and 'Training cost & time'. Initial values can be placeholders (TBD).",
            "dependencies": [],
            "details": "Deliverable: An updated `README.md` file. Acceptance Criteria: The file contains a new H2 section named 'Success Metrics' with a table or list defining the three key metrics. The definitions must be unambiguous.",
            "status": "pending",
            "testStrategy": "Review the `README.md` file to confirm the presence and clarity of the new 'Success Metrics' section and its definitions."
          },
          {
            "id": 2,
            "title": "Measure Base Model Performance",
            "description": "Run the existing evaluation script against the base model to generate its performance metrics, which will serve as the official baseline.",
            "dependencies": [],
            "details": "Deliverable: A new JSON file in `results/`. Command: `python eval/run_eval.py --model=base --output=results/baseline_metrics.json`. Acceptance Criteria: The file `results/baseline_metrics.json` is created and contains populated values for `hallucination_rate` and `avg_inference_time`.",
            "status": "pending",
            "testStrategy": "Run `ls results/baseline_metrics.json` to confirm file creation and inspect its contents for the required keys."
          },
          {
            "id": 3,
            "title": "Document Baseline Values in README",
            "description": "Update the 'Success Metrics' section in the README with the concrete baseline values obtained from the base model evaluation.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Deliverable: An updated `README.md`. Using the data from `results/baseline_metrics.json`, replace the 'TBD' baseline placeholders in the README. The baseline 'Win-rate vs. base' is 0% by definition. Acceptance Criteria: The 'Baseline' column/section in the README's metrics table is fully populated with specific numbers.",
            "status": "pending",
            "testStrategy": "Review the `README.md` to confirm that no 'TBD' placeholders remain for baseline metrics."
          },
          {
            "id": 4,
            "title": "Set and Document Target Values",
            "description": "Based on the established baseline, determine and document the final target values for all success metrics in the README.",
            "dependencies": [
              "4.3"
            ],
            "details": "Deliverable: An updated `README.md`. Decide on ambitious but achievable targets (e.g., Win-rate > 70%, Hallucination rate < 5%). Update the 'Target' column/section in the README's metrics table. Acceptance Criteria: The 'Target' column/section is populated with specific, non-TBD values.",
            "status": "pending",
            "testStrategy": "Review the `README.md` to confirm that all target metrics have specific, quantitative values."
          },
          {
            "id": 5,
            "title": "Create Centralized Metrics Config",
            "description": "Create a configuration file to store the target metrics, allowing evaluation scripts to programmatically access them for pass/fail checks.",
            "dependencies": [
              "4.4"
            ],
            "details": "Deliverable: A new configuration file at `eval/targets.yaml`. This file should contain the target values defined in the README. Acceptance Criteria: The file `eval/targets.yaml` exists and its key-value pairs (e.g., `win_rate_target: 0.7`) match the documented targets.",
            "status": "pending",
            "testStrategy": "Run `cat eval/targets.yaml` to verify its contents are correct and match the `README.md`."
          }
        ]
      },
      {
        "id": 5,
        "title": "Develop Fine-Tuning Pipeline",
        "description": "Create the core scripts in the `src/` directory to fine-tune the selected LLM on the chosen dataset using PEFT/LoRA and BitsAndBytes.",
        "details": "This is the central training logic of the project. The pipeline must be runnable and produce a fine-tuned model adapter that can be used for evaluation and demos.",
        "testStrategy": "Execute the training script. Verify that it runs to completion and outputs a model artifact (e.g., LoRA adapter weights) to a designated directory.",
        "priority": "high",
        "dependencies": [
          1,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Data Loading and Preprocessing",
            "description": "Create a script to load the dataset and prepare it for training by applying a prompt template and tokenizing the text.",
            "dependencies": [],
            "details": "Deliverable: `src/data_loader.py`. This script must contain a function that returns a tokenized `datasets.Dataset` object. Acceptance Criteria: The function successfully processes a sample of the dataset into the format required by the Trainer.",
            "status": "pending",
            "testStrategy": "python -c \"from src.data_loader import load_and_prepare_data; ds = load_and_prepare_data(split='train[:1%]'); print(ds[0])\""
          },
          {
            "id": 2,
            "title": "Configure Model and Tokenizer Loading",
            "description": "Create a utility to load the base LLM with 4-bit quantization via BitsAndBytes and its corresponding tokenizer.",
            "dependencies": [],
            "details": "Deliverable: `src/model_loader.py`. This script must provide a function that returns a `model` object loaded in 4-bit and a `tokenizer` object with correct padding settings. Acceptance Criteria: The function successfully loads the model and tokenizer without configuration errors.",
            "status": "pending",
            "testStrategy": "python -c \"from src.model_loader import load_model_and_tokenizer; load_model_and_tokenizer()\""
          },
          {
            "id": 3,
            "title": "Define LoRA and Training Configuration",
            "description": "Define PEFT LoRA settings and Hugging Face `TrainingArguments` in a central configuration script for reusability.",
            "dependencies": [],
            "details": "Deliverable: `src/config.py`. This file must contain a `LoraConfig` object and a `TrainingArguments` object. The output directory in `TrainingArguments` must be set to `./model_adapter`. Acceptance Criteria: The configuration objects can be imported and inspected without error.",
            "status": "pending",
            "testStrategy": "python -c \"from src.config import TRAINING_ARGS; print(TRAINING_ARGS.output_dir)\""
          },
          {
            "id": 4,
            "title": "Implement Core Training Script",
            "description": "Create the main executable script that integrates data, model, and configuration to run the fine-tuning process.",
            "dependencies": [],
            "details": "Deliverable: `src/train.py`. This script must use the components from previous subtasks to initialize a `SFTTrainer`, run the training loop, and save the resulting LoRA adapter. Acceptance Criteria: The script runs to completion and saves the adapter weights to the `./model_adapter` directory.",
            "status": "pending",
            "testStrategy": "python src/train.py --max_steps 10"
          },
          {
            "id": 5,
            "title": "Add Basic Training Pipeline Test",
            "description": "Create a simple integration test to verify that the training script executes without crashing and produces an output artifact.",
            "dependencies": [],
            "details": "Deliverable: `tests/test_train.py`. The test will execute `src/train.py` for a minimal number of steps (e.g., 2). Acceptance Criteria: The test passes, confirming the script runs successfully and the `./model_adapter` directory is created.",
            "status": "pending",
            "testStrategy": "pytest tests/test_train.py"
          }
        ]
      },
      {
        "id": 6,
        "title": "Develop Evaluation Scripts",
        "description": "Implement scripts in the `eval/` directory to run the base and fine-tuned models against the evaluation set and generate metrics.",
        "details": "Fulfills FR-2 and NFR-1. The scripts must be reproducible and save results to the `results/` directory in CSV and/or JSON format. This includes logic for calculating win-rate based on the defined rubric.",
        "testStrategy": "Execute the documented evaluation script commands. Verify that new result files (CSV/JSON) are created in the `results/` directory and their structure matches the defined format.",
        "priority": "high",
        "dependencies": [
          1,
          4,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Evaluation Runner Skeleton",
            "description": "Establish the basic structure for the evaluation script. This script will handle argument parsing (e.g., model paths, dataset path) and load the evaluation dataset and a specified model (either base or fine-tuned).",
            "dependencies": [],
            "details": "Create `eval/run_eval.py` with argparse for configuration. Implement logic to load the evaluation dataset from `data/processed/` and initialize a model and tokenizer using its Hugging Face ID or local path.",
            "status": "pending",
            "testStrategy": "Run `python eval/run_eval.py --model_id <base_model_id> --limit 1`. Verify the script executes without error and logs that the model and dataset were loaded successfully."
          },
          {
            "id": 2,
            "title": "Implement Inference Loop",
            "description": "Add the core logic to the evaluation script to iterate through the evaluation dataset, generate responses from the loaded model for each prompt, and collect the results.",
            "dependencies": [
              "6.1"
            ],
            "details": "In `eval/run_eval.py`, write a loop that takes each entry from the evaluation set, formats it into a prompt, and uses the model's `generate()` method to produce a response. Store pairs of (prompt, response) in a list.",
            "status": "pending",
            "testStrategy": "Run `python eval/run_eval.py --model_id <base_model_id> --limit 5`. Verify that 5 responses are generated and printed to the console."
          },
          {
            "id": 3,
            "title": "Implement Win-Rate Metric Logic",
            "description": "Create a separate module to house the metric calculation logic. This includes the function to compare the base and fine-tuned model responses and determine a winner based on the defined rubric.",
            "dependencies": [],
            "details": "Create `eval/metrics.py`. Implement a function `calculate_win_rate(base_response, tuned_response)` that returns 'base_win', 'tuned_win', or 'tie'. For now, the rubric can be a simple heuristic (e.g., response length, keyword presence). Create `tests/test_metrics.py` to validate this logic.",
            "status": "pending",
            "testStrategy": "Run `pytest -q tests/test_metrics.py`. Verify all unit tests for the win-rate calculation pass."
          },
          {
            "id": 4,
            "title": "Orchestrate Full Evaluation Run",
            "description": "Expand the main script to run the evaluation for both the base and fine-tuned models sequentially. The script should now collect responses from both models for each prompt in the evaluation set.",
            "dependencies": [
              "6.2"
            ],
            "details": "Refactor `eval/run_eval.py` to load the base model, run inference, then load the fine-tuned model adapter and run inference again. Store results in a structured list of dictionaries, e.g., `[{'prompt': p, 'base_response': r1, 'tuned_response': r2}, ...]`. This depends on Task 5 providing the fine-tuned adapter path.",
            "status": "pending",
            "testStrategy": "Run `python eval/run_eval.py --base_model_id <id> --tuned_model_path <path> --limit 2`. Verify the script logs that it is running inference for both models and collects paired responses."
          },
          {
            "id": 5,
            "title": "Calculate and Save Final Results",
            "description": "Integrate the win-rate logic into the main script and save the detailed per-example results and a final summary report to the `results/` directory.",
            "dependencies": [
              "6.3",
              "6.4"
            ],
            "details": "In `eval/run_eval.py`, after generating all responses, iterate through the results, call the `calculate_win_rate` function from `eval/metrics.py` for each pair, and append the outcome. Save the full, detailed list to `results/evaluation_results.json`. Calculate aggregate stats (win-rate, ties) and save them to `results/evaluation_summary.json`.",
            "status": "pending",
            "testStrategy": "Run `python eval/run_eval.py --base_model_id <id> --tuned_model_path <path>`. Verify that `results/evaluation_results.json` and `results/evaluation_summary.json` are created and contain the expected data structures and metrics."
          }
        ]
      },
      {
        "id": 7,
        "title": "Integrate Cost and Time Observability",
        "description": "Instrument the fine-tuning and evaluation pipelines to log and report on training cost and wall-clock time.",
        "details": "Fulfills NFR-3. The logged cost and time data should be saved to the `results/` directory alongside the quality metrics for each run.",
        "testStrategy": "After running the training and evaluation scripts, inspect the output files in `results/` to confirm that wall-clock time and/or cost metrics are present and correctly formatted.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Wall-Clock Timer to Training Script",
            "description": "Instrument the main training script to measure the total execution time from start to finish.",
            "dependencies": [],
            "details": "In `src/train.py`, use the `time` module to record the start time before the training process begins and the end time after the model adapter is saved. The duration should be calculated and logged to the console.",
            "status": "pending",
            "testStrategy": "Run `python src/train.py --config <config_file>`. Verify that a message like 'Total training time: HH:MM:SS' is printed to standard output upon completion."
          },
          {
            "id": 2,
            "title": "Persist Training Time and Cost to Results",
            "description": "Save the measured training time and an estimated cost to a structured file in the results directory.",
            "dependencies": [
              "7.1"
            ],
            "details": "Modify `src/train.py` to write a JSON file to `results/training_metrics_<run_id>.json`. This file should contain `wall_time_seconds` (from subtask 7.1) and `estimated_cost_usd`. The cost should be calculated by multiplying the duration by a configurable hourly rate for the target GPU (e.g., $1.10/hr for an A10G).",
            "status": "pending",
            "testStrategy": "Run the training script. Check for the creation of a new `training_metrics_*.json` file in the `results/` directory. Inspect the file to confirm it contains the correct keys and plausible numeric values."
          },
          {
            "id": 3,
            "title": "Add Wall-Clock Timer to Evaluation Script",
            "description": "Instrument the main evaluation script to measure the total execution time.",
            "dependencies": [],
            "details": "In `eval/run_eval.py`, use the `time` module to record the start time before model loading and the end time after all metrics have been calculated and saved. Log the total duration to the console.",
            "status": "pending",
            "testStrategy": "Run `python eval/run_eval.py --model_path <path_to_adapter>`. Verify that a message like 'Total evaluation time: MM:SS' is printed to standard output upon completion."
          },
          {
            "id": 4,
            "title": "Append Evaluation Time to Metrics File",
            "description": "Incorporate the measured evaluation time directly into the primary evaluation results file.",
            "dependencies": [
              "7.3"
            ],
            "details": "Modify `eval/run_eval.py` to add the `wall_time_seconds` measured in subtask 7.3 to the summary section of the main results JSON or as a new column in the results CSV being saved to the `results/` directory.",
            "status": "pending",
            "testStrategy": "Run the evaluation script. Inspect the generated `eval_results_*.json` or `.csv` file in `results/` to confirm the presence and correctness of the `wall_time_seconds` field."
          },
          {
            "id": 5,
            "title": "Test Observability Output Generation",
            "description": "Add unit tests to verify that the observability metrics files are created with the correct format and data types.",
            "dependencies": [
              "7.2",
              "7.4"
            ],
            "details": "In `tests/test_pipelines.py`, add a new test case. This test should mock the time-consuming training/evaluation functions but call the actual metric-saving logic. Assert that the output files are created in a temporary directory and that their contents (e.g., keys like `wall_time_seconds`, `estimated_cost_usd`) match the expected schema.",
            "status": "pending",
            "testStrategy": "Run `pytest -q tests/test_pipelines.py`. Confirm that all tests, including the new one for observability, pass."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement Baseline Correctness Tests",
        "description": "Create a test suite in the `tests/` directory that can be run with `pytest` to validate core behaviors.",
        "details": "Fulfills FR-3. The tests should cover critical functions within the `src/` and `eval/` modules to guard against regressions.",
        "testStrategy": "Run `pytest -q` from the project's root directory and confirm that the command returns a successful exit code and all tests pass.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Test Suite with Pytest",
            "description": "Create the initial test file and configuration to ensure the `pytest` framework is operational within the project.",
            "dependencies": [],
            "details": "Create a `tests/` directory if one does not exist. Add a `tests/test_sanity.py` file containing a simple passing test, like `def test_assert_true(): assert True`. This confirms that the test runner is discovered and executed correctly.",
            "status": "pending",
            "testStrategy": "Run `pytest -q` from the project root. The command must exit successfully and report that 1 test passed."
          },
          {
            "id": 2,
            "title": "Test Data Loading and Preprocessing",
            "description": "Verify that the data loading and tokenization functions in the `src/` module work correctly on a sample input.",
            "dependencies": [],
            "details": "Create `tests/test_data.py`. Add a test that uses a small, self-contained mock data sample. The test should call the data processing function and assert that the output (e.g., a dictionary or dataset object) has the correct keys (`input_ids`, `attention_mask`) and that the tensor shapes are as expected for a small batch.",
            "status": "pending",
            "testStrategy": "Run `pytest -q tests/test_data.py` and confirm it passes."
          },
          {
            "id": 3,
            "title": "Test Core Model Loading",
            "description": "Ensure the utility function for loading the base model and tokenizer from the `src/` module executes without errors.",
            "dependencies": [],
            "details": "Create `tests/test_model.py`. Write a test that calls the model loading utility (e.g., `load_model()`). The test should assert that the returned objects are of the expected Hugging Face `PreTrainedModel` and `PreTrainedTokenizer` (or similar) classes.",
            "status": "pending",
            "testStrategy": "Run `pytest -q tests/test_model.py` and confirm it passes."
          },
          {
            "id": 4,
            "title": "Test Single Training Step",
            "description": "Validate the core training logic by running a single forward and backward pass on a mock data batch to ensure mechanics are sound.",
            "dependencies": [
              "8.2",
              "8.3"
            ],
            "details": "Create `tests/test_train.py`. This test will instantiate a model, create a mock batch of data, perform one training step (forward pass, loss calculation, backward pass, optimizer step), and assert that a valid, non-null loss value is returned.",
            "status": "pending",
            "testStrategy": "Run `pytest -q tests/test_train.py` and confirm it passes."
          },
          {
            "id": 5,
            "title": "Test Evaluation Logic",
            "description": "Verify the correctness of the core metric calculation function from the `eval/` module using controlled inputs.",
            "dependencies": [
              "8.3"
            ],
            "details": "Create `tests/test_eval.py`. The test should define mock model predictions and corresponding ground-truth references. Pass these to the core evaluation function (e.g., `calculate_metrics()`) from `eval/run_eval.py` and assert that the resulting score matches a pre-calculated, expected value.",
            "status": "pending",
            "testStrategy": "Run `pytest -q tests/test_eval.py` and confirm it passes."
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement CLI Demo",
        "description": "Build the interactive command-line Q&A demo as specified in the PRD to allow for quick qualitative checks.",
        "details": "Fulfills FR-4. The `demo.py` script should launch an interface where a user can input a question and receive a response from the fine-tuned model.",
        "testStrategy": "Run `python demo.py`, enter at least one question at the prompt, and verify that a model-generated answer is printed to the console.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CLI Argument Parser",
            "description": "Set up the initial `demo.py` script with an argument parser to handle command-line options, specifically for the model path.",
            "dependencies": [],
            "details": "Use Python's `argparse` module to add a required `--model_path` argument. This ensures the demo script knows which fine-tuned model checkpoint to load. The deliverable is the initial version of `demo.py`.",
            "status": "pending",
            "testStrategy": "Run `python demo.py --help` and verify that the `--model_path` argument is listed in the usage instructions."
          },
          {
            "id": 2,
            "title": "Load Fine-Tuned Model and Tokenizer",
            "description": "Add logic to `demo.py` to load the fine-tuned PEFT model and its corresponding tokenizer from the path provided via the `--model_path` argument.",
            "dependencies": [
              "9.1"
            ],
            "details": "Implement functions to load the `AutoModelForCausalLM` with the PEFT adapter and the `AutoTokenizer`. The script should print a confirmation message like 'Model loaded successfully.' upon completion. Deliverable is the updated `demo.py`.",
            "status": "pending",
            "testStrategy": "Run `python demo.py --model_path <path_to_a_valid_model>` and confirm the script executes without errors and prints the success message."
          },
          {
            "id": 3,
            "title": "Build Interactive Input Loop",
            "description": "Implement a `while` loop in `demo.py` that continuously prompts the user for input and allows them to exit gracefully.",
            "dependencies": [
              "9.1"
            ],
            "details": "Create a loop that presents a user prompt (e.g., `Question: `). The loop should break if the user types a specific command like 'exit' or 'quit'. Deliverable is the updated `demo.py` with the interactive loop structure.",
            "status": "pending",
            "testStrategy": "Run `python demo.py --model_path <path_to_model>`. Verify the prompt appears. Enter text and press Enter; the prompt should reappear. Type 'exit' and press Enter; the script should terminate."
          },
          {
            "id": 4,
            "title": "Implement Inference Logic",
            "description": "Within the input loop, add the core logic to take the user's text input, tokenize it, and pass it to the loaded model to generate a response.",
            "dependencies": [
              "9.2",
              "9.3"
            ],
            "details": "Use the loaded tokenizer to encode the user's input string into token IDs. Pass these IDs to the model's `generate()` method to get the output tokens. Deliverable is the updated `demo.py` with generation logic.",
            "status": "pending",
            "testStrategy": "Add a temporary `print()` statement to show the raw generated token IDs. Run the demo, enter a question, and verify that a tensor of token IDs is printed to the console."
          },
          {
            "id": 5,
            "title": "Display Formatted Response",
            "description": "Decode the model's generated token IDs back into human-readable text and print the formatted answer to the console.",
            "dependencies": [
              "9.4"
            ],
            "details": "Use the tokenizer's `decode()` method on the output from the `generate()` call. Print the resulting string to the console, prefixed with a label like `Answer: `. The final deliverable is the completed `demo.py` script.",
            "status": "pending",
            "testStrategy": "Run `python demo.py --model_path <path_to_model>`. Enter a question (e.g., 'What is the capital of France?') and verify that a coherent, human-readable answer is printed to the console."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement FastAPI Demo",
        "description": "Create a FastAPI web service demo to showcase a service-style interface for the fine-tuned model.",
        "details": "Fulfills FR-5. The application should expose at least one endpoint that accepts a JSON request with a query and returns a JSON response with the model's answer.",
        "testStrategy": "Start the FastAPI server. Use a tool like `curl` or a web browser to send a request to the documented endpoint and verify that a valid JSON response is received.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Basic FastAPI App",
            "description": "Initialize the FastAPI application file with a root endpoint to confirm the server runs.",
            "dependencies": [],
            "details": "Create `demo/app.py`. Import `FastAPI` and instantiate the app. Add a simple root GET endpoint that returns `{\"message\": \"OK\"}`. This ensures the basic server setup is correct before adding model logic.",
            "status": "pending",
            "testStrategy": "Run `uvicorn demo.app:app --reload`. Access `http://127.0.0.1:8000` in a browser or with `curl` to verify the `OK` message is returned."
          },
          {
            "id": 2,
            "title": "Define API Data Models",
            "description": "Define Pydantic models for the request and response bodies to ensure type safety and auto-generate documentation.",
            "dependencies": [
              "10.1"
            ],
            "details": "In `demo/app.py`, create a `QueryRequest` Pydantic model with a single field `query: str`. Create a `QueryResponse` model with a field `answer: str`. These will structure the API's I/O.",
            "status": "pending",
            "testStrategy": "Code review and linter pass on `demo/app.py` to confirm valid Pydantic models are defined. No runnable command."
          },
          {
            "id": 3,
            "title": "Implement Prediction Endpoint Stub",
            "description": "Create the `/predict` POST endpoint using the data models, returning a hardcoded response.",
            "dependencies": [
              "10.2"
            ],
            "details": "In `demo/app.py`, add a `/predict` endpoint that accepts a POST request with a `QueryRequest` body and returns a `QueryResponse`. For now, the implementation should return a fixed string like `{\"answer\": \"This is a placeholder response.\"}`.",
            "status": "pending",
            "testStrategy": "Run `uvicorn demo.app:app`. Send a request using `curl -X POST 'http://127.0.0.1:8000/predict' -H 'Content-Type: application/json' -d '{\"query\": \"test\"}'`. Verify the hardcoded JSON response is returned."
          },
          {
            "id": 4,
            "title": "Integrate Model Inference Logic",
            "description": "Connect the fine-tuned model to the endpoint to generate live predictions.",
            "dependencies": [
              "10.3"
            ],
            "details": "Modify the `/predict` endpoint in `demo/app.py`. Add logic to load the fine-tuned model and tokenizer (dependency from Task 5). Use the model to process the input `query` from the request and generate a real answer. The model should be loaded once on startup to avoid reloading on every request.",
            "status": "pending",
            "testStrategy": "Run `uvicorn demo.app:app`. Send a request using `curl` as in the previous step. Verify the response contains a plausible, model-generated answer, not the placeholder text."
          },
          {
            "id": 5,
            "title": "Add API Test and README",
            "description": "Create a `pytest` test for the API endpoint and document its usage in the main README.",
            "dependencies": [
              "10.4"
            ],
            "details": "Create `tests/test_demo_api.py`. Use FastAPI's `TestClient` to write a test that sends a request to the `/predict` endpoint and asserts a 200 status code and a valid response format. Update `README.md` with a section on how to run the FastAPI demo, including the `uvicorn` and `curl` commands.",
            "status": "pending",
            "testStrategy": "Run `pytest -q tests/test_demo_api.py` and confirm it passes. Manually inspect the `README.md` to verify the new instructions are clear and correct."
          }
        ]
      },
      {
        "id": 11,
        "title": "Generate and Document Before/After Examples",
        "description": "Create qualitative examples comparing the base model's answers to the fine-tuned model's answers to visually demonstrate improvements.",
        "details": "Fulfills FR-6. Add a section to the README or a demo page with at least one clear, side-by-side before/after comparison for a sample tech-support question.",
        "testStrategy": "Review the repository's README or demo documentation to confirm the presence of at least one before/after example pair.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Sample Prompts",
            "description": "Create a file containing a curated list of at least 3 diverse tech-support questions to be used for generating before/after examples. These prompts will serve as the input for both the base and fine-tuned models.",
            "dependencies": [],
            "details": "Create a new file `eval/sample_prompts.txt`. Add at least three representative tech-support questions, with each question on a new line. The questions should be realistic and designed to highlight the fine-tuned model's improvements.",
            "status": "pending",
            "testStrategy": "Run `cat eval/sample_prompts.txt` and verify it contains at least three non-empty lines."
          },
          {
            "id": 2,
            "title": "Script Base Model Inference",
            "description": "Create a script that loads the base model, reads prompts from `eval/sample_prompts.txt`, and saves the generated, un-tuned answers to a markdown file.",
            "dependencies": [
              "11.1"
            ],
            "details": "Implement `eval/generate_base_output.py`. This script will iterate through `eval/sample_prompts.txt`, pass each prompt to the base model, and write the output to `results/base_model_examples.md`.",
            "status": "pending",
            "testStrategy": "Run `python eval/generate_base_output.py`. Check that `results/base_model_examples.md` is created and contains answers corresponding to the sample prompts."
          },
          {
            "id": 3,
            "title": "Script Fine-Tuned Model Inference",
            "description": "Create a script that loads the fine-tuned model, reads prompts from `eval/sample_prompts.txt`, and saves the generated, tuned answers to a markdown file.",
            "dependencies": [
              "11.1"
            ],
            "details": "Implement `eval/generate_tuned_output.py`. This script will iterate through `eval/sample_prompts.txt`, pass each prompt to the fine-tuned model (from Task 6), and write the output to `results/tuned_model_examples.md`.",
            "status": "pending",
            "testStrategy": "Run `python eval/generate_tuned_output.py`. Check that `results/tuned_model_examples.md` is created and contains answers corresponding to the sample prompts."
          },
          {
            "id": 4,
            "title": "Assemble Comparison Table",
            "description": "Create a script that reads the outputs from the base and fine-tuned models and assembles them into a single, side-by-side markdown table for easy comparison.",
            "dependencies": [
              "11.2",
              "11.3"
            ],
            "details": "Implement `eval/create_comparison.py`. This script will read `results/base_model_examples.md` and `results/tuned_model_examples.md` and generate a new file, `results/before_after_comparison.md`, containing a markdown table with columns: 'Prompt', 'Base Model Answer', 'Fine-Tuned Model Answer'.",
            "status": "pending",
            "testStrategy": "Run `python eval/create_comparison.py`. Verify that `results/before_after_comparison.md` exists and contains a well-formatted markdown table."
          },
          {
            "id": 5,
            "title": "Update Project README",
            "description": "Add a new 'Model Performance Examples' section to the main `README.md` and embed the content from the generated comparison file.",
            "dependencies": [
              "11.4"
            ],
            "details": "Manually edit `README.md` to add a new H2 section titled 'Model Performance Examples'. Copy and paste the markdown table from `results/before_after_comparison.md` into this new section.",
            "status": "pending",
            "testStrategy": "Review the `README.md` file in a markdown viewer to confirm the new section and side-by-side comparison table are present and correctly formatted."
          }
        ]
      },
      {
        "id": 12,
        "title": "Setup CI Pipeline",
        "description": "Configure a Continuous Integration (CI) workflow to automatically run tests and linters on code changes.",
        "details": "Fulfills FR-9. The CI pipeline should be configured to run on pull requests to the main branch and report a pass/fail status, for example, as a status check in GitHub.",
        "testStrategy": "Create a pull request with a minor change. Verify that the CI pipeline is triggered automatically and that it reports a 'success' status after running the tests.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CI Workflow File",
            "description": "Create the basic GitHub Actions workflow file at `.github/workflows/ci.yml`. The workflow must trigger on pull requests targeting the `main` branch and include placeholder jobs for linting and testing that check out the repository code.",
            "dependencies": [],
            "details": "Create the directory `.github/workflows/` if it doesn't exist. Add a `ci.yml` file with an `on: pull_request: branches: [ main ]` trigger. Define `lint` and `test` jobs that use `actions/checkout@v4`.",
            "status": "pending",
            "testStrategy": "Open a new pull request against the `main` branch. Verify that the 'CI' workflow is automatically triggered in the GitHub 'Actions' tab and shows the two defined jobs."
          },
          {
            "id": 2,
            "title": "Implement Linting Job",
            "description": "Flesh out the 'lint' job in `.github/workflows/ci.yml`. This job must install dependencies from `requirements.txt` and run a linter (e.g., ruff) across the codebase. The CI pipeline must fail if any linting errors are found.",
            "dependencies": [
              "12.1"
            ],
            "details": "In the `lint` job, add steps to set up Python, install dependencies via `pip install -r requirements.txt`, and then run the linter. Use the command: `ruff check .`",
            "status": "pending",
            "testStrategy": "Run the linter locally to ensure it passes before pushing: `ruff check .`. Intentionally introduce a style error, push to a PR, and verify the CI job fails as expected."
          },
          {
            "id": 3,
            "title": "Implement Testing Job",
            "description": "Flesh out the 'test' job in `.github/workflows/ci.yml`. This job must install dependencies from `requirements.txt` and execute the project's test suite located in the `tests/` directory. The CI pipeline must fail if any test fails.",
            "dependencies": [
              "12.1"
            ],
            "details": "In the `test` job, add steps to set up Python, install dependencies from `requirements.txt`, and run the test suite using the command: `pytest tests/`.",
            "status": "pending",
            "testStrategy": "Run the test suite locally to ensure it passes before pushing: `pytest tests/`. Create a temporary failing test in a new file under `tests/`, push to a PR, and verify the CI job fails."
          },
          {
            "id": 4,
            "title": "Configure Dependency Caching",
            "description": "Update the 'lint' and 'test' jobs in `.github/workflows/ci.yml` to cache Python package dependencies. This will speed up subsequent workflow runs by avoiding re-downloading packages from PyPI.",
            "dependencies": [
              "12.2",
              "12.3"
            ],
            "details": "Before the 'Install dependencies' step in each job, add a step using the `actions/cache@v4` action. Configure it to cache the `pip` cache directory, using a key based on the runner's OS and the hash of `requirements.txt`.",
            "status": "pending",
            "testStrategy": "After merging, trigger the workflow twice on the same branch. Check the logs for the 'Cache Dependencies' step. The first run should report a cache miss, and the second run should report a cache hit, resulting in a faster job execution time."
          },
          {
            "id": 5,
            "title": "Enforce PR Status Checks",
            "description": "Configure a branch protection rule for `main` to require that the 'lint' and 'test' CI jobs pass before a pull request can be merged. This ensures code quality and correctness on the main branch.",
            "dependencies": [
              "12.2",
              "12.3"
            ],
            "details": "In the GitHub repository settings, navigate to Branches > Branch protection rules. Add a rule for the `main` branch. Enable 'Require status checks to pass before merging' and select the 'lint' and 'test' jobs from the list of available checks.",
            "status": "pending",
            "testStrategy": "Open a pull request that fails one of the CI jobs (e.g., a test fails). Verify that the GitHub UI shows a red 'X' for the status check and that the 'Merge pull request' button is disabled with a message indicating the required check has failed."
          }
        ]
      },
      {
        "id": 13,
        "title": "Add License and Compliance Documentation",
        "description": "Add the MIT LICENSE file and ensure any required notices for upstream dependencies are included for legal compliance.",
        "details": "Fulfills FR-11 and NFR-5. A `LICENSE` file containing the MIT License text must be present in the repository root. The README should also state the license.",
        "testStrategy": "Check for the existence of a `LICENSE` file in the repository root and verify its content is the MIT license. Check the README for the license statement.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create MIT LICENSE File",
            "description": "Generate the standard MIT License text and place it in a `LICENSE` file at the project root.",
            "dependencies": [],
            "details": "Create a new file named `LICENSE` in the repository's root directory. Copy the full, standard text of the MIT License into this file, replacing '[year]' and '[fullname]' with the appropriate project details.",
            "status": "pending",
            "testStrategy": "Run `ls -a | grep LICENSE` to confirm file existence. Run `cat LICENSE` to verify its content is the standard MIT license text."
          },
          {
            "id": 2,
            "title": "Update README with License Section",
            "description": "Add a 'License' section to the main `README.md` file to declare the project's license.",
            "dependencies": [
              "13.1"
            ],
            "details": "Edit `README.md` to include a new top-level section titled '## License'. The section should state that the project is licensed under the MIT License and include a link to the `LICENSE` file, e.g., `[MIT License](LICENSE)`.",
            "status": "pending",
            "testStrategy": "Review the rendered `README.md` to confirm the 'License' section exists and the link to the `LICENSE` file is functional."
          },
          {
            "id": 3,
            "title": "Scan Dependencies for Licenses",
            "description": "Identify all upstream dependencies from `requirements.txt` and document their respective licenses to assess compliance obligations.",
            "dependencies": [],
            "details": "Use a license scanning tool to generate a report of all packages listed in `requirements.txt` and their licenses. The output should be saved for review.",
            "status": "pending",
            "testStrategy": "Run `pip-licenses --from=mixed --format=markdown --output-file=docs/DEPENDENCY_LICENSES.md`. Verify the generated file `docs/DEPENDENCY_LICENSES.md` lists all dependencies and their license types."
          },
          {
            "id": 4,
            "title": "Create NOTICES File for Attributions",
            "description": "Based on the dependency scan, create a `NOTICES.md` file to include any required copyright notices and license texts from upstream dependencies.",
            "dependencies": [
              "13.3"
            ],
            "details": "Create a `NOTICES.md` file in the repository root. For any dependency whose license requires attribution (e.g., Apache 2.0, BSD), copy its license and/or copyright notice into this file. If no dependencies require notices, the file should state that.",
            "status": "pending",
            "testStrategy": "Review `NOTICES.md` to confirm it contains the necessary attributions as identified in the dependency scan from subtask 13.3."
          },
          {
            "id": 5,
            "title": "Add License Check to CI Pipeline",
            "description": "Update the CI workflow to automatically verify the presence and content of license-related files.",
            "dependencies": [
              "13.1",
              "13.4"
            ],
            "details": "Modify the CI configuration file (e.g., `.github/workflows/ci.yml`) to add a new step or job. This step will check that the `LICENSE` and `NOTICES.md` files exist and are not empty.",
            "status": "pending",
            "testStrategy": "Commit the updated CI configuration and push to a new branch. Create a pull request and verify that the new 'License Check' job runs and passes successfully."
          }
        ]
      },
      {
        "id": 14,
        "title": "Document Project Limitations and Future Work",
        "description": "Write the 'Limitations & Next Steps' section in the README to transparently communicate the project's scope and potential future directions.",
        "details": "Fulfills FR-10. This section should clearly outline what the project does not do and suggest areas for future improvement or expansion.",
        "testStrategy": "Review the `README.md` file to confirm that a section titled 'Limitations & Next Steps' or similar exists and contains relevant content.",
        "priority": "medium",
        "dependencies": [
          11,
          12
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Section Header to README",
            "description": "Create the 'Limitations & Next Steps' section in the main project documentation file to serve as a container for the subsequent content.",
            "dependencies": [],
            "details": "Edit the `README.md` file and add a new H2 markdown header: `## Limitations & Next Steps`. This establishes the section's location and title.",
            "status": "pending",
            "testStrategy": "Visually inspect the `README.md` file to confirm the new header exists."
          },
          {
            "id": 2,
            "title": "List Current Project Limitations",
            "description": "Document the known boundaries and constraints of the current implementation to set clear expectations for users and contributors.",
            "dependencies": [
              "14.1"
            ],
            "details": "Under the new header in `README.md`, create a sub-header `### Limitations` and add a bulleted list of at least three specific limitations. Examples: model's knowledge is restricted to the training data's scope, it only handles single-turn interactions, or it has a known hallucination rate based on evaluation.",
            "status": "pending",
            "testStrategy": "Review `README.md` to confirm a 'Limitations' subsection exists and contains at least three distinct, clearly explained points."
          },
          {
            "id": 3,
            "title": "Outline Potential Future Work",
            "description": "Suggest concrete areas for improvement and expansion to guide future development efforts and showcase the project's potential.",
            "dependencies": [
              "14.1"
            ],
            "details": "In `README.md`, create a sub-header `### Next Steps` and add a bulleted list of at least three actionable ideas for future work. Examples: training on a larger, more diverse dataset; experimenting with a larger base model (e.g., Llama-3 70B); or implementing a continuous evaluation pipeline.",
            "status": "pending",
            "testStrategy": "Review `README.md` to confirm a 'Next Steps' subsection exists and contains at least three specific, forward-looking suggestions."
          },
          {
            "id": 4,
            "title": "Connect Limitations to Evaluation Data",
            "description": "Ground the documented limitations in concrete data by referencing the project's evaluation results and qualitative examples.",
            "dependencies": [
              "14.2"
            ],
            "details": "Update the text within the 'Limitations' subsection in `README.md`. Explicitly reference the metrics defined in Task 4 (e.g., 'Hallucination rate') and the qualitative comparisons from Task 11 to provide evidence for the stated limitations. This makes the documentation more credible and data-driven.",
            "status": "pending",
            "testStrategy": "Verify that the 'Limitations' text in `README.md` contains at least one reference to a specific metric (e.g., 'win-rate') or a before/after example."
          },
          {
            "id": 5,
            "title": "Review and Format Final Section",
            "description": "Perform a final review of the entire 'Limitations & Next Steps' section to ensure it is clear, concise, and well-formatted.",
            "dependencies": [
              "14.3",
              "14.4"
            ],
            "details": "Proofread the completed section in `README.md` for grammatical errors and typos. Ensure consistent and readable markdown formatting (e.g., bullet points, bolding). Confirm the final text transparently communicates the project's scope and fulfills functional requirement FR-10.",
            "status": "pending",
            "testStrategy": "Read the final section in `README.md` from the perspective of a new user to check for clarity, accuracy, and professional presentation."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-24T17:48:18.562Z",
      "updated": "2025-08-24T17:48:18.562Z",
      "description": "Tasks for master context"
    }
  }
}