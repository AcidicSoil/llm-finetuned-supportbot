{
  "default": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Project Repository and Environment",
        "description": "Initialize the project structure, virtual environment, and core dependencies.",
        "details": "Create the repository structure including directories like `src/`, `scripts/`, `configs/`, `eval/`, `results/`, `tests/`, and `api/`. Initialize a uv project with `pyproject.toml` declaring runtime dependencies (`transformers`, `datasets`, `peft`, `trl`, `accelerate`, `bitsandbytes`, `torch`) and a `dev` dependency group containing `pytest`.",
        "testStrategy": "Verify that the directory structure is created, `pyproject.toml` exists, and `uv sync --dev` completes successfully.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Define Data Schema and Validation Logic",
        "description": "Establish a unified schema for all input data and implement validators to ensure data quality.",
        "details": "Implement a data model class or Pydantic model for the schema: `{id, inputs:{question, context?}, outputs:{answer}, meta:{source, timestamp, tags[]}}`. Create validation functions to check for required fields, data types, and non-empty content.",
        "testStrategy": "Write unit tests using `pytest` (run via `uv run pytest`) to check that the validator correctly accepts valid data and rejects invalid or incomplete data.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Core Pydantic Models for the Data Schema",
            "description": "Create the fundamental, nested Pydantic models for the unified data structure, including `Inputs`, `Outputs`, `Meta`, and the main `DataRecord`.",
            "dependencies": [],
            "details": "Implement the Pydantic models as specified: `Inputs` with `question` (str) and optional `context` (str). `Outputs` with `answer` (str). `Meta` with `source` (str), `timestamp` (datetime), and `tags` (List[str]). Combine these into a main `DataRecord` model with a unique `id` (str).\n<info added on 2025-08-30T16:10:06.818Z>\nVerification complete: Core Pydantic models (`Inputs`, `Outputs`, `Meta`, `DataRecord`) are implemented in `src/models.py` as specified. Basic non-empty validators using Pydantic v2's `field_validator` are included. Auto-generated documentation (`SCHEMA.md`) and a JSON schema (`schema/data_schema.json`) have been created. Unit tests are written in `tests/test_models.py`, but were only statically inspected as the environment currently lacks the `pytest` dependency.\n</info added on 2025-08-30T16:10:06.818Z>",
            "status": "done",
            "testStrategy": "Write a basic unit test to ensure a dictionary with the correct structure and types can be successfully parsed into a `DataRecord` object without errors."
          },
          {
            "id": 2,
            "title": "Implement Advanced Field-Level Pydantic Validators",
            "description": "Enhance the Pydantic models with custom validators to enforce stricter data quality rules beyond basic type checking.",
            "dependencies": [
              "2.1"
            ],
            "details": "Use Pydantic's `@validator` decorator to add validation logic. Ensure that string fields like `id`, `question`, `answer`, and `source` are not empty or just whitespace. Validate that the `tags` list, if not empty, contains only non-empty strings.",
            "status": "done",
            "testStrategy": "Using `pytest` (run via `uv run pytest`), create tests that attempt to initialize the `DataRecord` model with invalid data (e.g., an empty `question`, a `tag` with an empty string) and assert that a `ValidationError` is raised."
          },
          {
            "id": 3,
            "title": "Develop Dataset-Level Validation Logic",
            "description": "Create functions to validate an entire collection of data records, checking for inter-record consistency and potential issues like duplicates or PII.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "Implement a function `validate_dataset(records: List[DataRecord])` that iterates through a list of records. This function should check for duplicate `id` values. Add a placeholder for a PII scan on text fields. If a controlled vocabulary for tags is required, validate that all tags conform to the allowed set.",
            "status": "done",
            "testStrategy": "Create a sample list of `DataRecord` objects containing a duplicate ID and another list with a disallowed tag. Write unit tests to confirm that the `validate_dataset` function correctly identifies and flags these issues."
          },
          {
            "id": 4,
            "title": "Establish and Integrate Schema Versioning",
            "description": "Incorporate a versioning system into the schema to manage future changes and ensure long-term maintainability and backward compatibility.",
            "dependencies": [
              "2.1"
            ],
            "details": "Add a `schema_version: str` field to the main `DataRecord` model, setting a default value like \"1.0\". This allows data parsers and other downstream processes to identify the schema version they are handling.",
            "status": "done",
            "testStrategy": "Verify through unit tests that newly created `DataRecord` objects are automatically assigned the correct default `schema_version`. Test that data with a missing or different version can be identified."
          },
          {
            "id": 5,
            "title": "Generate Schema Documentation with Examples",
            "description": "Produce both machine-readable and human-readable documentation for the schema to guide developers, especially those working on data parsers (Task 3).",
            "dependencies": [
              "2.1",
              "2.2",
              "2.4"
            ],
            "details": "Use the Pydantic model's `.schema_json()` method to generate a formal `data_schema.json` file. Create a `SCHEMA.md` markdown file that explains each field, its type, and validation rules. Include clear JSON examples of one valid and one invalid record to illustrate usage.",
            "status": "done",
            "testStrategy": "Manually review the generated `data_schema.json` to ensure it accurately reflects the Pydantic model. Review `SCHEMA.md` for clarity, correctness, and the utility of its examples."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Data Parsers for Various Sources",
        "description": "Create parsers to load support dialogs, tickets, and Q&A data from different formats into the unified schema.",
        "details": "Develop modular parser functions within the `src/` directory, each responsible for a specific data source (e.g., JSON, CSV). Each parser should output a list of objects conforming to the defined data schema.",
        "testStrategy": "Test each parser with sample input files and assert that the output is correctly structured and validated by the schema validator.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement Stratified Train/Val/Test Splitting",
        "description": "Create a script to split the unified dataset into training, validation, and test sets to prevent data leakage.",
        "details": "The splitting logic should be deterministic and allow for stratification based on metadata fields like `source` or `tags` to ensure balanced representation across splits.",
        "testStrategy": "Verify that the script produces distinct train, validation, and test files. Check the distribution of stratified keys across the splits to ensure it's balanced.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Data Tokenization",
        "description": "Convert the preprocessed text data into tokenized format suitable for the model.",
        "details": "Use the tokenizer from the chosen base model (via `transformers` library). The process should handle padding and truncation of sequences up to `max_seq_length`.",
        "testStrategy": "Tokenize a sample batch of data and inspect the output `input_ids` and `attention_mask` to ensure they have the correct shape and format.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Create Master Data Preparation Script",
        "description": "Orchestrate the entire data ingestion and preprocessing pipeline with a single command.",
        "details": "Create `scripts/prepare_data.py` that takes input/output paths as arguments. This script will call the parsers, validator, splitter, and tokenizer in sequence to generate the final processed datasets.",
        "testStrategy": "Run the script with sample raw data and verify that the final processed and tokenized `train`, `val`, and `test` datasets are created in the specified output directory.",
        "priority": "high",
        "dependencies": [
          3,
          4,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement PEFT/LoRA Fine-Tuning Loop",
        "description": "Set up the core training logic to fine-tune a small open LLM using LoRA with quantization.",
        "details": "Use the Hugging Face `Trainer` or TRL's `SFTTrainer`. Integrate `peft` to create a LoRA model and `bitsandbytes` for 4/8-bit quantization. Ensure the training loop is reproducible using deterministic seeds.",
        "testStrategy": "Run a short training job (1-2 steps) on a small data sample. Verify that it completes without errors and a model checkpoint is saved.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Load Base Model and Tokenizer with Quantization",
            "description": "Load the specified small open LLM and its tokenizer from the Hugging Face Hub, applying 4/8-bit quantization during model instantiation.",
            "dependencies": [],
            "details": "Use `transformers.AutoTokenizer.from_pretrained` to load the tokenizer. Use `transformers.AutoModelForCausalLM.from_pretrained` with a `bitsandbytes.BitsAndBytesConfig` to load the model in 4-bit (e.g., NF4) or 8-bit mode. This combines the model selection and quantization steps.",
            "status": "done",
            "testStrategy": "Verify that the model object is an instance of a quantized model and the tokenizer can process a sample string."
          },
          {
            "id": 2,
            "title": "Configure and Apply LoRA Adapters",
            "description": "Define the LoRA configuration and apply it to the quantized base model to make it trainable.",
            "dependencies": [
              "7.1"
            ],
            "details": "Create a `peft.LoraConfig` object, specifying the rank (`r`), `lora_alpha`, `target_modules` (e.g., `['q_proj', 'v_proj']`), and `task_type`. Use `peft.get_peft_model` to wrap the quantized model from the previous step.",
            "status": "done",
            "testStrategy": "Inspect the model's parameters and confirm that only the LoRA adapter weights are marked as trainable."
          },
          {
            "id": 3,
            "title": "Set Up Training Arguments for Reproducibility and Checkpointing",
            "description": "Configure the `transformers.TrainingArguments` to ensure the training loop is deterministic and saves checkpoints periodically.",
            "dependencies": [],
            "details": "Instantiate `TrainingArguments`, setting a fixed `seed` for reproducibility. Configure `output_dir`, `per_device_train_batch_size`, `num_train_epochs`, `logging_steps`, and `save_strategy` (e.g., 'steps') along with `save_steps` to enable checkpointing.",
            "status": "done",
            "testStrategy": "Review the arguments object to ensure the seed and checkpointing parameters are correctly set before passing to the trainer."
          },
          {
            "id": 4,
            "title": "Initialize and Run the SFTTrainer Loop",
            "description": "Instantiate the TRL `SFTTrainer` with the PEFT model, dataset, and training arguments, then start the training process.",
            "dependencies": [
              "7.2",
              "7.3"
            ],
            "details": "Initialize `trl.SFTTrainer`, passing the PEFT model, training/evaluation datasets (from Task 5), the tokenizer, and the configured `TrainingArguments`. Call the `trainer.train()` method to start the fine-tuning loop.",
            "status": "done",
            "testStrategy": "Run the training script for a small number of steps (e.g., 1-2) on a tiny dataset sample. Verify it completes without errors and a checkpoint directory is created."
          },
          {
            "id": 5,
            "title": "Implement Logic to Resume from a Checkpoint",
            "description": "Modify the training script to handle a command-line argument that allows resuming training from a specified checkpoint directory.",
            "dependencies": [
              "7.4"
            ],
            "details": "Add logic (e.g., using `argparse`) to detect a `--resume-from-checkpoint` argument. If present, pass its value (the path to a checkpoint) to the `resume_from_checkpoint` parameter of the `trainer.train()` method.",
            "status": "done",
            "testStrategy": "Start a training run for 2 steps, stop it, then restart it with the `--resume-from-checkpoint` flag pointing to the 'checkpoint-2' directory. Verify from the logs that training resumes from step 3."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement Configuration Management for Training",
        "description": "Allow training hyperparameters to be specified via configuration files and command-line arguments.",
        "details": "Use YAML files (e.g., `configs/sft.yaml`) to define training parameters like learning rate, epochs, LoRA rank/alpha, etc. Use a library like `argparse` or `jsonargparse` to parse the config and allow CLI overrides.",
        "testStrategy": "Launch a training run using a config file. Override a parameter (e.g., `num_train_epochs`) via the CLI and check the console logs to confirm the override was applied.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Implement Checkpoint Saving and Resuming",
        "description": "Enable saving model checkpoints during training and resuming training from a saved checkpoint.",
        "details": "Configure the `Trainer` to save checkpoints periodically. Implement logic in the training script (`scripts/train.py`) to handle a `--resume-from-checkpoint` argument.",
        "testStrategy": "Start a training run, stop it after one epoch, then resume it from the saved checkpoint. Verify that the training continues from the correct step/epoch.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Develop Evaluation Harness",
        "description": "Create a script to run a fine-tuned model and a base model against a predefined evaluation question suite.",
        "details": "Create `scripts/eval.py` that loads a fine-tuned model (adapter) and a base model. It should iterate through a suite of questions (e.g., `eval/suites/support_100.json`) and generate answers from both models.",
        "testStrategy": "Run the script on a trained model and a base model with a small 5-question suite. Verify that it generates outputs for both models for all questions.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Base model and tokenizer loader util",
            "description": "Utility to load a base model and tokenizer for evaluation.",
            "details": "Create `src/eval_utils.py:load_base(model_name_or_path: str, device: str)` that returns `(model, tokenizer)` using `AutoModelForCausalLM` and `AutoTokenizer`. Ensure `torch_dtype`, `device_map`, and `padding_side='left'` are set appropriately for generation. Test with a tiny model.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 2,
            "title": "PEFT adapter load/apply util",
            "description": "Utility to load a PEFT adapter and apply to a base model.",
            "details": "Extend `src/eval_utils.py` with `load_with_adapter(base_model_name: str, adapter_path: str, device: str)` that loads base, applies `PeftModel.from_pretrained` (or `get_peft_model` + `PeftConfig.from_pretrained`), and returns `(model, tokenizer)`. Validate weights are loaded and set to eval.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 3,
            "title": "CLI entrypoint scripts/eval.py",
            "description": "Main evaluation script with argparse for paths and options.",
            "details": "Create `scripts/eval.py` with arguments: `--base`, `--adapter`, `--suite`, `--out`, `--max-new-tokens`, `--temperature`, `--device`. Select loader based on presence of `--adapter`. Use tqdm logging.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 4,
            "title": "Evaluation loop for base vs adapter",
            "description": "Generate responses from base and fine-tuned models over the suite.",
            "details": "Implement a loop that reads JSON/JSONL suite entries `{question, context?}`; formats prompts; runs `generate` on base and adapter models with fixed seeds; collects outputs with latency. Keep batch size small and handle device placement.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 5,
            "title": "Save paired generations to JSONL/CSV",
            "description": "Write structured outputs with question/context, base, fine-tuned, and timing.",
            "details": "Add writers to emit JSONL (one record per item) and optional CSV. Fields: `id`, `question`, `context`, `base.text`, `adapter.text`, `base.latency_ms`, `adapter.latency_ms`. Validate files exist and contain expected keys.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Automated Metrics Calculation",
        "description": "Calculate win-rate and hallucination rate based on the evaluation harness outputs.",
        "details": "For side-by-side comparison, implement a win-rate test (can be manual review initially or use a stronger LLM as a judge). Create a stub for a rubric-based hallucination scoring system. The script should aggregate these scores.",
        "testStrategy": "Provide sample model outputs to the metric script and verify that it calculates and aggregates the scores correctly.",
        "priority": "high",
        "dependencies": [
          10
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Results Exporter and Summary Table",
        "description": "Save detailed evaluation results to a file and print a summary table to the console.",
        "details": "The evaluation script should output detailed per-question results to a CSV or JSON file in the `results/` directory. It should also print a formatted summary table to the console showing win rate, hallucination rate, and other key metrics.",
        "testStrategy": "Run the evaluation script and check that a correctly formatted CSV/JSON file is created in `results/` and that a summary table is printed to standard output.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Create Interactive CLI Demo",
        "description": "Build a simple command-line interface for quick Q&A with the fine-tuned model.",
        "details": "Create `demo.py` that loads a trained model and enters a loop, prompting the user for a question and printing the model's generated answer.",
        "testStrategy": "Run `uv run demo.py`, enter a question, and verify that the model generates a response. Ensure a clean exit path exists.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Develop FastAPI Inference Service",
        "description": "Create a web service for model inference.",
        "details": "In the `api/` directory, create a minimal FastAPI application. Implement a `POST /generate` endpoint that accepts a prompt and returns a generated text, and a `GET /healthz` endpoint that returns a status.",
        "testStrategy": "Run the app with `uv run uvicorn`. Send a `GET` request to `/healthz` and a `POST` request to `/generate` using `curl` or another client. Verify the responses are correct.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold FastAPI App with Health and Generate Endpoints",
            "description": "Create the initial file structure in the `api/` directory. Set up a minimal FastAPI application in `api/main.py` with two endpoints: a `GET /healthz` that returns a 200 OK status, and a placeholder `POST /generate` that returns a hardcoded response.",
            "dependencies": [],
            "details": "Initialize the FastAPI app object. Implement the `/healthz` endpoint to return `{\"status\": \"ok\"}`. The `/generate` endpoint can initially return a static JSON object like `{\"generated_text\": \"placeholder response\"}`.",
            "status": "done",
            "testStrategy": "Run `uv run uvicorn api.main:app` and send a GET request to `/healthz` to verify the status response. Send a POST request to `/generate` to verify the placeholder response."
          },
          {
            "id": 2,
            "title": "Define Pydantic Request/Response Models",
            "description": "Create and integrate Pydantic models for the `/generate` endpoint's request and response bodies to ensure type-safe data handling.",
            "dependencies": [
              "14.1"
            ],
            "details": "In `api/main.py` or a new `api/models.py`, define a `GenerateRequest` model with a `prompt` field (string). Define a `GenerateResponse` model with a `generated_text` field (string). Update the `/generate` endpoint to use these models in its signature.",
            "status": "done",
            "testStrategy": "With the app running, send a POST request to `/generate` with a valid JSON body (`{\"prompt\": \"hello\"}`). Verify it succeeds. Send a request with an invalid body (e.g., wrong field name) and verify FastAPI returns a 422 Unprocessable Entity error."
          },
          {
            "id": 3,
            "title": "Implement API Key Authentication Stub",
            "description": "Add a simple, non-production-ready authentication mechanism that requires a static API key to be present in the request headers for the `/generate` endpoint.",
            "dependencies": [
              "14.1"
            ],
            "details": "Create a dependency function that checks for an `X-API-Key` header. If the header is missing or its value doesn't match a predefined static key (e.g., from an environment variable), raise an `HTTPException` with a 401 status. Apply this dependency to the `/generate` endpoint.",
            "status": "done",
            "testStrategy": "Send a request to `/generate` without the `X-API-Key` header and verify a 401/403 error. Send a request with the correct header and verify a 200 OK response."
          },
          {
            "id": 4,
            "title": "Enhance Generate Endpoint with Batching Support",
            "description": "Update the `/generate` endpoint and its models to support both single-prompt and batch-prompt requests.",
            "dependencies": [
              "14.2"
            ],
            "details": "Modify the `GenerateRequest` model to accept either a single string or a list of strings for the `prompt` field (e.g., using `Union[str, List[str]]`). Update the `GenerateResponse` to return a corresponding single string or list of strings. The endpoint logic should handle both cases, iterating through prompts if a list is provided.",
            "status": "done",
            "testStrategy": "Test the `/generate` endpoint with a single prompt request. Then, test it with a batch request containing multiple prompts in a list and verify the response contains a corresponding list of generated texts."
          },
          {
            "id": 5,
            "title": "Create Run Script and README Documentation",
            "description": "Create a simple run script to start the service and add a README file in the `api/` directory with setup and usage instructions.",
            "dependencies": [
              "14.1"
            ],
            "details": "Create a `run.sh` script containing the command `uv run uvicorn api.main:app --host 0.0.0.0 --port 8000`. Create an `api/README.md` file detailing how to install dependencies (e.g., `uv sync --dev`), how to run the service using the script, and providing `curl` examples for both the `/healthz` and `/generate` endpoints.",
            "status": "done",
            "testStrategy": "Execute the `run.sh` script and verify the server starts. Follow the instructions in the new `README.md` to test the endpoints with `curl` and confirm they work as documented."
          }
        ]
      },
      {
        "id": 15,
        "title": "Implement Model Packaging Utility",
        "description": "Create a utility to save the trained model artifacts for deployment.",
        "details": "The utility should save the LoRA adapter weights, the tokenizer configuration, and any other necessary model config files in the standard Hugging Face format. Create a corresponding simple loader function.",
        "testStrategy": "Package a trained model. Then, use the loader utility in a separate script (like the demo or API) to load the packaged artifacts and confirm it can perform inference.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Write Unit Tests for Data Pipeline",
        "description": "Create unit tests for the data loading, validation, and splitting components.",
        "details": "Using `pytest`, write tests in the `tests/` directory to cover the functions in the data processing modules. Use mock data to test edge cases.",
        "testStrategy": "Run `uv run pytest tests/` and ensure all data-related tests pass.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Write Smoke Tests for Training and Inference",
        "description": "Create fast-running tests to ensure the main training and inference paths are not broken.",
        "details": "Create a `tests/smoke` directory. The training smoke test should run for a single step on a tiny dataset. The inference smoke test should load a model and generate one response.",
        "testStrategy": "Run the smoke tests as part of the main test suite. They should complete in under 60 seconds and pass.",
        "priority": "high",
        "dependencies": [
          7,
          13
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Setup Basic CI Pipeline",
        "description": "Configure a continuous integration pipeline to run checks on code changes.",
        "details": "Create a GitHub Actions workflow (or similar) that triggers on push/pull_request. The pipeline should install dependencies, run linters/style checkers (e.g., ruff), and execute the `pytest` suite including unit and smoke tests.",
        "testStrategy": "Push a commit or open a pull request and verify that the CI pipeline runs and passes successfully.",
        "priority": "high",
        "dependencies": [
          16,
          17
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Add Configurable Training Recipes (SFT/DPO)",
        "description": "Extend the training script to support different training objectives like DPO.",
        "details": "Refactor the training script to allow selection of a trainer (e.g., `SFTTrainer`, `DPOTrainer` from TRL) via the YAML configuration. This requires adapting the data format for preference data.",
        "testStrategy": "Run a short training job using a new `dpo.yaml` config and verify it uses the `DPOTrainer`.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add DPO data loader/mapper",
            "description": "Extend data pipeline to load preference data with fields {prompt, chosen, rejected} and map to TRL DPOTrainer expected format.",
            "details": "Implement loader that reads JSON/JSONL preference items and returns a Dataset with columns ['prompt','chosen','rejected']. Provide a small utility to validate records and to convert from our existing schema if needed.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 2,
            "title": "Select trainer via config (SFT vs DPO)",
            "description": "Update training entry to choose SFTTrainer or DPOTrainer based on a config flag.",
            "details": "Add a config key like recipe: sft|dpo. When dpo, instantiate TRL DPOTrainer and DPOConfig (wire beta and shared args). Keep SFT path unchanged.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 3,
            "title": "Add DPO config file",
            "description": "Create configs/dpo.yaml with DPO-specific fields and doc differences vs sft.yaml.",
            "details": "Include fields: recipe: dpo, model/splits/output, beta, max_length, batch sizes, logging/eval settings. Add README snippet on switching recipes.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 4,
            "title": "DPO smoke test in CI",
            "description": "Create tiny DPO dataset and minimal config to run a single training step and assert DPO path executes.",
            "details": "Add a pytest that launches `scripts/train.py --config configs/dpo.yaml` on a tiny preference set, 1 step, asserting logs mention DPOTrainer and job completes.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          }
        ]
      },
      {
        "id": 20,
        "title": "Implement Best Model Selection",
        "description": "Automatically save the best model checkpoint based on a specified evaluation metric.",
        "details": "Configure the Hugging Face `Trainer` with `load_best_model_at_end=True` and specify `metric_for_best_model` (e.g., 'eval_loss' or a custom win-rate metric).",
        "testStrategy": "Run a training and evaluation job. Verify that the final saved checkpoint corresponds to the epoch with the best score on the specified evaluation metric.",
        "priority": "medium",
        "dependencies": [
          9,
          11
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Expand Evaluation Suite and Error Analysis",
        "description": "Add more evaluation questions and tools for qualitative error analysis.",
        "details": "Curate additional evaluation sets for different domains. Enhance the results output to include before/after examples and categorize errors into buckets (e.g., 'wrong_fact', 'out_of_scope').",
        "testStrategy": "Run the enhanced evaluation and check that the output includes the new error buckets and qualitative examples.",
        "priority": "medium",
        "dependencies": [
          10
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Curate additional evaluation sets",
            "description": "Add at least two new JSON/JSONL evaluation suites for different support domains.",
            "details": "Create `eval/suites/support_billing.jsonl` and `eval/suites/support_setup.jsonl` with fields `{id, inputs:{question, context?}, outputs:{answer?}, meta:{tags[]}}`. Ensure coverage of varied intents and difficulty.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 21
          },
          {
            "id": 2,
            "title": "Define error classification schema and rubric",
            "description": "Introduce structured error categories and a rubric for annotation.",
            "details": "Add `src/eval_schema.py` with `Enum ErrorType { REFUSAL, HALLUCINATION, STYLE, OTHER }` and rubric notes in `eval/README.md`. Provide examples and decision rules.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 21
          },
          {
            "id": 3,
            "title": "Annotate eval outputs with error category",
            "description": "Modify evaluation to capture error categories per response.",
            "details": "Extend `scripts/eval.py` to optionally apply a rule-based tagger (using the rubric) and add `error_type` to each record. Leave hooks for manual review or future ML-based tagging.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 21
          },
          {
            "id": 4,
            "title": "Aggregate and report error statistics",
            "description": "Update results summary to include error-type aggregates.",
            "details": "Extend results writer to compute counts and rates per `ErrorType` and print a summary table. Save an `error_stats.json` with aggregates per suite and overall.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 21
          }
        ]
      },
      {
        "id": 22,
        "title": "Implement Advanced Input Length Handling",
        "description": "Add strategies for handling inputs that are longer than the model's context window.",
        "details": "Implement an optional sliding-window strategy for long contexts as an alternative to simple truncation. This can be configured in the data processing step.",
        "testStrategy": "Process a document longer than the max sequence length and verify that the sliding window approach generates multiple overlapping chunks.",
        "priority": "medium",
        "dependencies": [
          5,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define chunking config options",
            "description": "",
            "details": "Add config keys: chunking.strategy = [\"truncate\",\"sliding_window\"], chunking.max_seq_length, chunking.stride. Wire into configs/sft.yaml and dpo.yaml with sensible defaults.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 22
          },
          {
            "id": 2,
            "title": "Implement sliding-window chunker",
            "description": "",
            "details": "Add a reusable function (e.g., src/chunking.py) that splits text into overlapping chunks using max_seq_length and stride; returns token ids and attention masks ready for model input.",
            "status": "done",
            "dependencies": [
              "22.1"
            ],
            "parentTaskId": 22
          },
          {
            "id": 3,
            "title": "Integrate chunking into preprocessing & inference",
            "description": "",
            "details": "Use the chunker in scripts/tokenize_dataset.py and in inference (api/main.py or demo.py) so both training and inference can handle long inputs consistently.",
            "status": "done",
            "dependencies": [
              "22.2"
            ],
            "parentTaskId": 22
          },
          {
            "id": 4,
            "title": "Unit tests for windowing and edge cases",
            "description": "",
            "details": "Add tests that verify: (1) over-length inputs produce multiple chunks with expected overlap; (2) exactly-at-limit yields one chunk; (3) tiny inputs; (4) masks and shapes are valid.",
            "status": "done",
            "dependencies": [
              "22.2"
            ],
            "parentTaskId": 22
          },
          {
            "id": 5,
            "title": "CLI/docs wiring",
            "description": "",
            "details": "Expose a CLI flag to choose chunking strategy and stride; document tradeoffs in README and configs. Ensure CI passes with new tests.",
            "status": "done",
            "dependencies": [
              "22.3",
              "22.4"
            ],
            "parentTaskId": 22
          }
        ]
      },
      {
        "id": 23,
        "title": "Add Mixed-Precision and Gradient Accumulation Presets",
        "description": "Provide pre-configured settings for common performance optimization techniques.",
        "details": "Add options to the training config (YAML) to easily enable/disable mixed-precision (`fp16`/`bf16`) and set the number of gradient accumulation steps.\n<info added on 2025-09-03T13:14:41.842Z>\nInitial investigation started. Reviewing documentation for mixed-precision (`fp16`/`bf16`) and gradient accumulation. Inspecting the existing configuration files and training script entrypoints to outline the implementation plan for the preset system.\n</info added on 2025-09-03T13:14:41.842Z>",
        "testStrategy": "Run training with `gradient_accumulation_steps=4` and verify from the logs that the effective batch size is 4x the per-device batch size.",
        "priority": "low",
        "dependencies": [
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define preset YAMLs",
            "description": "",
            "details": "Add preset overlays under configs/presets/: cpu.yaml (no AMP, ga=1), gpu-bf16.yaml (bf16 on, ga=1), gpu-fp16.yaml (fp16 on, ga=1), memory-efficient.yaml (reduce batch size, increase gradient_accumulation_steps).",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 23
          },
          {
            "id": 2,
            "title": "Add --preset overlay flag",
            "description": "",
            "details": "Update scripts/train_lora.py to accept --preset <name>. If provided, load configs/presets/<name>.yaml and apply as defaults before CLI overrides.",
            "status": "done",
            "dependencies": [
              "23.1"
            ],
            "parentTaskId": 23
          },
          {
            "id": 3,
            "title": "Auto-detect precision support (optional)",
            "description": "",
            "details": "If --preset not provided, optionally auto-enable bf16 when torch.cuda.is_available() and torch.cuda.get_device_capability>= (8,0) and bf16 supported; else leave as configured.",
            "status": "done",
            "dependencies": [
              "23.2"
            ],
            "parentTaskId": 23
          },
          {
            "id": 4,
            "title": "Tests: preset overlay precedence",
            "description": "",
            "details": "Unit test that --preset loads YAML overlay and CLI flags still override. Verify bf16/fp16 and gradient_accumulation_steps values in args snapshot prior to trainer init.",
            "status": "done",
            "dependencies": [
              "23.2"
            ],
            "parentTaskId": 23
          },
          {
            "id": 5,
            "title": "Docs: presets quickstart",
            "description": "",
            "details": "Add README section with examples for presets and a table of recommended combos (CPU, single-GPU bf16, older-GPU fp16, memory-efficient).",
            "status": "done",
            "dependencies": [
              "23.1"
            ],
            "parentTaskId": 23
          }
        ]
      },
      {
        "id": 24,
        "title": "Add Hyperparameter Sweeps and Early Stopping",
        "description": "Integrate logic for hyperparameter optimization and early stopping to prevent overfitting.",
        "details": "Integrate with a library like Optuna or Ray Tune for sweeping LoRA rank/alpha. Implement early stopping callback in the `Trainer` to halt training if the validation metric stops improving.",
        "testStrategy": "Configure an early stopping callback with `patience=2`. Run a training job where the validation loss plateaus and verify that training stops early.",
        "priority": "low",
        "dependencies": [
          8,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate EarlyStoppingCallback",
            "description": "Wire EarlyStoppingCallback into training and expose patience/metric in config.",
            "details": "Add config fields {early_stopping_patience, metric_for_best_model, greater_is_better}. Pass load_best_model_at_end=True and register EarlyStoppingCallback with patience.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 24
          },
          {
            "id": 2,
            "title": "Create sweep runner (Optuna)",
            "description": "Add scripts/sweep.py with Optuna objective that launches training and returns selected metric.",
            "details": "Define objective sampling space for lr, lora_r, lora_dropout, gradient_accumulation_steps. Run N trials and write the best params/score to results/sweeps/.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 24
          },
          {
            "id": 3,
            "title": "Define search space via config",
            "description": "Allow search ranges in YAML to drive sweep (e.g., learning_rate:[1e-5,1e-3], lora_r:[4,64])",
            "details": "Update config parsing to accept a sweep section with ranges/choices; sweep runner reads it to build the Optuna search space.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 24
          },
          {
            "id": 4,
            "title": "Docs for sweeps + early stopping",
            "description": "Document how to run a local sweep and interpret results; include early stopping usage.",
            "details": "README section with example commands for scripts/sweep.py, environment tips, and how to read best trial output; note patience/metric config and CI-friendly defaults.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 24
          }
        ]
      },
      {
        "id": 25,
        "title": "Create Lightweight Model Registry",
        "description": "Implement a simple file-based model registry to track experiments and artifacts.",
        "details": "Create a top-level `registry/` directory. After each successful evaluation, write a metadata JSON file to this directory containing the run ID, metrics, and path to the packaged model artifacts.",
        "testStrategy": "Run a full train-and-eval pipeline. Verify that a new metadata file is created in the `registry/` directory with the correct information.",
        "priority": "low",
        "dependencies": [
          15
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Metadata Schema for Registry Entry",
            "description": "Establish a clear, consistent structure for the metadata that will be saved for each model.",
            "dependencies": [],
            "details": "Create a Pydantic model or a formal dictionary structure in a new `registry/schema.py` file. The schema must include fields for `run_id` (string), `evaluation_metrics` (dictionary), `model_artifact_path` (string), and a `creation_timestamp` (string, ISO 8601 format).",
            "status": "pending",
            "testStrategy": "Review the schema definition to ensure it captures all required information as per the task description."
          },
          {
            "id": 2,
            "title": "Implement Registry Directory Management",
            "description": "Ensure the top-level `registry/` directory is automatically created if it does not exist.",
            "dependencies": [],
            "details": "In the main training script or a shared utility module, add a function that is called at startup. This function will use `pathlib.Path.mkdir(parents=True, exist_ok=True)` to create the `registry/` directory at the project root.",
            "status": "pending",
            "testStrategy": "Delete the `registry/` directory and run the training script. Verify the directory is created."
          },
          {
            "id": 3,
            "title": "Create Metadata Writer Function",
            "description": "Develop a dedicated function to handle the serialization and writing of a metadata entry to a JSON file.",
            "dependencies": [
              "25.1",
              "25.2"
            ],
            "details": "In a new `registry/utils.py` module, create a function `write_metadata(metadata: dict)`. This function will take the metadata, generate a filename from the `run_id` (e.g., `{run_id}.json`), and write the data to a file in the `registry/` directory in a human-readable JSON format.",
            "status": "pending",
            "testStrategy": "Call the function with mock metadata and verify that the corresponding JSON file is created with the correct content and filename in the `registry/` directory."
          },
          {
            "id": 4,
            "title": "Integrate Metadata Writing into Evaluation Pipeline",
            "description": "Trigger the metadata writing process upon the successful completion of a model evaluation.",
            "dependencies": [
              "25.3"
            ],
            "details": "Locate the point in the evaluation script where final metrics are calculated. After a successful run, collect the current run ID, the computed metrics dictionary, and the path to the saved model artifact. Pass this information to the `write_metadata` function.",
            "status": "pending",
            "testStrategy": "Run a partial training and evaluation cycle. Check that the `write_metadata` function is called with the correct arguments at the end of evaluation."
          },
          {
            "id": 5,
            "title": "Verify Registry Entry Creation in a Full Run",
            "description": "Confirm that a complete train-and-eval pipeline correctly produces a new registry file.",
            "dependencies": [
              "25.4"
            ],
            "details": "Execute the full train-and-eval pipeline as defined in the parent task's test strategy. After completion, manually inspect the `registry/` directory for a new JSON file corresponding to the completed run.",
            "status": "pending",
            "testStrategy": "Check the newly created JSON file's contents to ensure the `run_id`, `evaluation_metrics`, and `model_artifact_path` are accurate and match the results of the pipeline run."
          }
        ]
      },
      {
        "id": 26,
        "title": "Implement Prompt Templating System",
        "description": "Allow the use of different prompt templates for inference to guide model behavior.",
        "details": "Create a system (e.g., using Jinja2) that allows users to define and select prompt templates. The inference endpoints (CLI and API) should apply the selected template to the user's input.",
        "testStrategy": "Use the CLI demo with a specific prompt template and verify the final prompt sent to the model is formatted correctly.",
        "priority": "low",
        "dependencies": [
          13,
          14
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Jinja2 as a dependency",
            "description": "Introduce templating support for prompts.",
            "details": "Add `jinja2` to project dependencies (pyproject/requirements) and lock. Verify import and simple template render.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 2,
            "title": "Create prompt templates directory",
            "description": "Provide example templates for common formats.",
            "details": "Add `templates/` with `chatml.j2` and `alpaca.j2`. Include variables for `{question}` and optional `{context}`. Document usage in `templates/README.md`.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 3,
            "title": "Implement apply_template utility",
            "description": "Helper to render a named template with data.",
            "details": "Create `src/templates.py` with `apply_template(name: str, data: dict) -> str` that loads from `templates/` and renders via Jinja2. Add simple caching and error handling for missing templates.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 4,
            "title": "Integrate templating into CLI demo",
            "description": "Allow selecting a template for prompt formatting in the CLI.",
            "details": "Update `demo.py` to accept `--template` and apply `apply_template` with `{question, context?}` before generation. Fallback to raw prompt when omitted.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 5,
            "title": "Integrate templating into FastAPI endpoint",
            "description": "Support optional template parameter in `/generate`.",
            "details": "Extend `api` service request model to include optional `template`. Use `apply_template` to build the final prompt for both single and batch requests.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          }
        ]
      },
      {
        "id": 27,
        "title": "Add Optional Guardrail Hooks",
        "description": "Implement hooks to check for and filter unsafe or out-of-scope queries and responses.",
        "details": "In the inference logic, add an optional pre-processing hook for input queries and a post-processing hook for model responses. These hooks can apply simple checks like regex or keyword matching.",
        "testStrategy": "Enable a guardrail that blocks a specific keyword. Send a request containing that keyword to the API and verify that it is rejected or handled appropriately.",
        "priority": "low",
        "dependencies": [
          14
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Guardrail Hook Interface",
            "description": "Establish a clear, reusable interface for both pre-processing and post-processing guardrail functions.",
            "dependencies": [],
            "details": "Define a base class or function signature. For pre-processing, it should accept a query string and return a boolean indicating safety and the (potentially modified) query. For post-processing, it should accept a response string and return the filtered/modified response.",
            "status": "pending",
            "testStrategy": "Review the interface definition to ensure it's flexible enough for various checks (keywords, regex, etc.) and can signal a rejection clearly."
          },
          {
            "id": 2,
            "title": "Integrate Pre-processing Hook into Inference Logic",
            "description": "Modify the inference pipeline to execute the optional input guardrail hook before sending the query to the model.",
            "dependencies": [
              "27.1"
            ],
            "details": "In the main inference function, add a check to see if a pre-processing hook is configured. If so, call it with the user's input query. If the hook flags the input as unsafe, immediately stop processing and return an appropriate error message.",
            "status": "pending",
            "testStrategy": "Temporarily hard-code a hook that always rejects input. Verify that API calls are blocked before they reach the model."
          },
          {
            "id": 3,
            "title": "Integrate Post-processing Hook into Inference Logic",
            "description": "Modify the inference pipeline to execute the optional output guardrail hook after the model generates a response.",
            "dependencies": [
              "27.1"
            ],
            "details": "After the model generates a response but before it's sent back to the user, add a check for a configured post-processing hook. Call the hook with the model's output and use its return value as the final response.",
            "status": "pending",
            "testStrategy": "Temporarily hard-code a hook that always replaces the model output with a fixed string like '[RESPONSE CENSORED]'. Verify that API calls return this fixed string."
          },
          {
            "id": 4,
            "title": "Implement a Basic Keyword/Regex Guardrail",
            "description": "Create a concrete implementation of the guardrail interface that filters content based on a configurable list of keywords or regex patterns.",
            "dependencies": [
              "27.1"
            ],
            "details": "Create a new function or class that accepts a list of banned keywords/patterns during initialization. Implement the hook logic to check if an input/output string contains any of the banned items. This will serve as the default, testable guardrail.",
            "status": "pending",
            "testStrategy": "Unit test the guardrail function directly. Pass strings that should be allowed and strings that should be blocked, and assert the correct behavior."
          },
          {
            "id": 5,
            "title": "Add Configuration to Enable and Test Guardrails",
            "description": "Expose configuration options to enable/disable guardrails and specify which hook functions to use.",
            "dependencies": [
              "27.2",
              "27.3",
              "27.4"
            ],
            "details": "Update the main configuration (e.g., YAML file or command-line arguments) to include parameters like `enable_guardrails`, `input_guardrail_path`, and `output_guardrail_path`. Load and apply these settings when the inference server starts.",
            "status": "pending",
            "testStrategy": "Use the new configuration to enable the keyword guardrail. Send an API request containing a blocked keyword and verify that the API rejects the request with an appropriate error message, as per the main task's Test Strategy."
          }
        ]
      },
      {
        "id": 28,
        "title": "Create Dockerfile for API Service",
        "description": "Package the FastAPI application and its dependencies into a Docker image for deployment.",
        "details": "Write a multi-stage `Dockerfile`. The first stage installs dependencies. The final stage copies the application code and model artifacts, exposing the API port. The entrypoint should run `uvicorn`.",
        "testStrategy": "Build the Docker image using `docker build`. Run the container and test the `/healthz` and `/generate` endpoints from the host machine.",
        "priority": "low",
        "dependencies": [
          14,
          15
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define the dependency builder stage",
            "description": "Create the first stage of the multi-stage Dockerfile to install Python dependencies, caching them in a separate layer.",
            "dependencies": [],
            "details": "Use a standard Python base image. Copy the `requirements.txt` file and run `pip install` to download and install all dependencies.",
            "status": "pending",
            "testStrategy": "Ensure this stage builds successfully without errors before proceeding to the next stage."
          },
          {
            "id": 2,
            "title": "Initialize the final runtime stage",
            "description": "Define the second, lightweight stage for the final image using a slim Python base.",
            "dependencies": [
              "28.1"
            ],
            "details": "Start a new stage from a slim base image like `python:3.11-slim`. Create and set a working directory for the application.",
            "status": "pending",
            "testStrategy": "Verify the Dockerfile syntax is correct for initiating a new stage."
          },
          {
            "id": 3,
            "title": "Copy dependencies and application code",
            "description": "Transfer the pre-installed dependencies from the builder stage and the API source code into the final runtime stage.",
            "dependencies": [
              "28.2"
            ],
            "details": "Use `COPY --from=builder ...` to copy the installed packages from the first stage. Add another `COPY` instruction for the application source code.",
            "status": "pending",
            "testStrategy": "Build the image and list files inside the container's working directory to verify that both the dependencies and source code are present."
          },
          {
            "id": 4,
            "title": "Add model artifacts and expose API port",
            "description": "Copy the necessary machine learning model files into the image and configure the network port.",
            "dependencies": [
              "28.3"
            ],
            "details": "Add a `COPY` instruction to place the model artifacts into a designated path within the container. Use the `EXPOSE` instruction to open the port the API will listen on.",
            "status": "pending",
            "testStrategy": "Run `docker inspect` on the built image to confirm the correct port is exposed."
          },
          {
            "id": 5,
            "title": "Set the Uvicorn entrypoint command",
            "description": "Define the command that will execute when the container starts, launching the FastAPI application.",
            "dependencies": [
              "28.4"
            ],
            "details": "Use the `CMD` instruction to specify the command to run the Uvicorn server, pointing to the FastAPI app instance and binding it to `0.0.0.0`.",
            "status": "pending",
            "testStrategy": "Run the container using `docker run`. Check the container logs to ensure Uvicorn starts successfully and is listening on the correct host and port."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-29T17:04:17.805Z",
      "updated": "2025-09-03T13:30:40.178Z",
      "description": "Tasks for default context"
    }
  }
}
